{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f785ce7e-435f-437b-bb61-f84cd07a799f",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Execution Timeline\n",
    "\n",
    "The **Execution Timeline** in LangGraph is a complete, time-ordered record of how a graph runs:\n",
    "**which nodes executed, in what order, with what state changes, and under which control decisions**.\n",
    "It is the primary tool for **debugging, observability, performance analysis, and correctness validation**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Why Execution Timeline Exists**\n",
    "\n",
    "LLM systems are **dynamic, non-linear, and stateful**.\n",
    "Traditional logs are insufficient.\n",
    "\n",
    "The execution timeline provides:\n",
    "\n",
    "| Capability          | Purpose                                |\n",
    "| ------------------- | -------------------------------------- |\n",
    "| Traceability        | Understand *why* a result was produced |\n",
    "| Debugging           | Locate faulty nodes or logic           |\n",
    "| Performance tuning  | Identify slow or expensive steps       |\n",
    "| Safety & compliance | Full audit trail                       |\n",
    "| Reproducibility     | Replay exact executions                |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Conceptual Model**\n",
    "\n",
    "Each graph run produces a timeline:\n",
    "\n",
    "```\n",
    "Time →\n",
    "┌─────┬──────────┬────────────┬────────────┬──────────────┐\n",
    "│Step │ Node     │ Input State│ Output State│ Decision     │\n",
    "├─────┼──────────┼────────────┼────────────┼──────────────┤\n",
    "│ 1   │ Planner  │ S0         │ S1         │ → Worker A   │\n",
    "│ 2   │ Worker A │ S1         │ S2         │ → Review     │\n",
    "│ 3   │ Review   │ S2         │ S3         │ → Worker B   │\n",
    "│ 4   │ Worker B │ S3         │ S4         │ → End        │\n",
    "└─────┴──────────┴────────────┴────────────┴──────────────┘\n",
    "```\n",
    "\n",
    "Every **state transition** is captured.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. What the Timeline Records**\n",
    "\n",
    "| Dimension         | Recorded Information           |\n",
    "| ----------------- | ------------------------------ |\n",
    "| Node execution    | Node name, type                |\n",
    "| Timing            | Start time, end time, duration |\n",
    "| State changes     | Before / after snapshots       |\n",
    "| Control decisions | Branch taken                   |\n",
    "| Errors            | Exceptions, retries            |\n",
    "| LLM data          | Prompt, response, token usage  |\n",
    "| Tool calls        | Arguments, results             |\n",
    "| Human input       | Approvals, edits               |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Timeline Generation in Practice**\n",
    "\n",
    "Enable tracing when invoking a graph:\n",
    "\n",
    "```python\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "\n",
    "tracer = LangChainTracer()\n",
    "\n",
    "result = graph.invoke(\n",
    "    {\"query\": \"Explain transformers\"},\n",
    "    config={\"callbacks\": [tracer]}\n",
    ")\n",
    "```\n",
    "\n",
    "This automatically records the full execution timeline.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Visualizing the Execution Timeline**\n",
    "\n",
    "Using **LangSmith**:\n",
    "\n",
    "1. Open LangSmith dashboard\n",
    "2. Select the run\n",
    "3. View **Execution Timeline**\n",
    "\n",
    "You see:\n",
    "\n",
    "* Node-by-node execution\n",
    "* Parallel branches\n",
    "* State diffs\n",
    "* Token & cost metrics\n",
    "* Errors and retries\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Timeline with Cycles and Branches**\n",
    "\n",
    "Cyclic graphs create repeating segments:\n",
    "\n",
    "```\n",
    "Reason → Act → Observe → Reason → Act → Observe → ...\n",
    "```\n",
    "\n",
    "Timeline representation:\n",
    "\n",
    "| Step | Node    | Iteration | Duration |\n",
    "| ---- | ------- | --------- | -------- |\n",
    "| 1    | Reason  | 1         | 120 ms   |\n",
    "| 2    | Act     | 1         | 300 ms   |\n",
    "| 3    | Observe | 1         | 90 ms    |\n",
    "| 4    | Reason  | 2         | 110 ms   |\n",
    "| ...  | ...     | ...       | ...      |\n",
    "\n",
    "Branches appear as **diverging execution paths** that later **rejoin**.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Performance Analysis Using Timeline**\n",
    "\n",
    "You can compute:\n",
    "\n",
    "* **Critical path**\n",
    "* Node latency distribution\n",
    "* Token cost per node\n",
    "* Throughput bottlenecks\n",
    "\n",
    "This enables:\n",
    "\n",
    "* Parallelization\n",
    "* Caching\n",
    "* Node refactoring\n",
    "* Model routing optimization\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Failure Analysis & Replay**\n",
    "\n",
    "Timeline enables:\n",
    "\n",
    "| Capability          | Description             |\n",
    "| ------------------- | ----------------------- |\n",
    "| Checkpointing       | Save intermediate state |\n",
    "| Replay              | Re-run from any step    |\n",
    "| Rollback            | Restore earlier state   |\n",
    "| Root cause analysis | Identify exact failure  |\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Production Use Cases**\n",
    "\n",
    "| Scenario            | Timeline Role                |\n",
    "| ------------------- | ---------------------------- |\n",
    "| Enterprise auditing | Compliance record            |\n",
    "| Incident debugging  | Post-mortem analysis         |\n",
    "| Model evaluation    | Compare strategies           |\n",
    "| Cost control        | Token & latency optimization |\n",
    "| Safety review       | Inspect agent decisions      |\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Mental Model**\n",
    "\n",
    "> **Execution Timeline = Flight Recorder of your AI system**\n",
    "\n",
    "It turns opaque LLM behavior into a **transparent, debuggable, auditable system**.\n",
    "\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66dc9bf-3bdb-48d9-a507-c78b3133f04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangouda\\Python3.1210\\Lib\\site-packages\\langsmith\\client.py:297: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019b6e0b-ace2-7b51-8c36-e85e002a608e,id=019b6e0b-ace2-7b51-8c36-e85e002a608e; trace=019b6e0b-ace2-7b51-8c36-e85e002a608e,id=019b6e0b-afe4-7f91-9bd7-7a78f27d92ca; trace=019b6e0b-ace2-7b51-8c36-e85e002a608e,id=019b6e0b-afe5-78f1-88c5-46c64e33b9e8\n",
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019b6e0b-ace2-7b51-8c36-e85e002a608e,id=019b6e0b-afe5-78f1-88c5-46c64e33b9e8; trace=019b6e0b-ace2-7b51-8c36-e85e002a608e,id=019b6e0b-afe4-7f91-9bd7-7a78f27d92ca; trace=019b6e0b-ace2-7b51-8c36-e85e002a608e,id=019b6e0b-d16b-77c2-9825-c57a09995ed5; trace=019b6e0b-ace2-7b51-8c36-e85e002a608e,id=019b6e0b-d16c-7ea1-9672-98f3b545cadd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Output ===\n",
      "\n",
      "The attention mechanism in transformers is a powerful tool that enables the model to assess the significance of different words within a sequence when making predictions or generating outputs. Unlike traditional models that process input data in a linear fashion, transformers leverage attention to evaluate all words in a sentence simultaneously. This capability allows them to capture intricate relationships and dependencies among words.\n",
      "\n",
      "At the heart of the attention mechanism is the computation of attention scores, which dictate the level of focus each word receives in relation to others. This process involves three primary components: queries, keys, and values.\n",
      "\n",
      "1. **Queries**: These vectors represent the current word or token for which the model seeks information from other words in the sequence.\n",
      "\n",
      "2. **Keys**: Each word in the input sequence is paired with a key vector. These keys help determine the relevance of each word to the current query.\n",
      "\n",
      "3. **Values**: Each word also has an associated value vector, which contains the information that will be conveyed based on the attention scores.\n",
      "\n",
      "To compute attention scores, the model takes the dot product of the query vector with the key vectors of all words. These scores are then normalized using a softmax function, resulting in a probability distribution that reflects the relative importance of each word. The final output is generated by calculating a weighted sum of the value vectors, where the weights correspond to the attention scores.\n",
      "\n",
      "This dynamic focus on various parts of the input sequence allows transformers to effectively capture long-range dependencies and contextual relationships. The self-attention mechanism, a defining feature of transformers, enables the model to attend to all words in the input sequence, making it particularly adept at tasks such as language translation, text summarization, and more.\n",
      "\n",
      "In summary, the attention mechanism in transformers significantly enhances the model's ability to comprehend context and relationships within data, leading to superior performance across a wide array of natural language processing tasks.\n",
      "\n",
      "Open LangSmith to view full Execution Timeline:\n",
      "https://smith.langchain.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019b6e0b-ace2-7b51-8c36-e85e002a608e,id=019b6e0b-d16c-7ea1-9672-98f3b545cadd; trace=019b6e0b-ace2-7b51-8c36-e85e002a608e,id=019b6e0b-d16b-77c2-9825-c57a09995ed5; trace=019b6e0b-ace2-7b51-8c36-e85e002a608e,id=019b6e0b-ace2-7b51-8c36-e85e002a608e\n"
     ]
    }
   ],
   "source": [
    "# ====== Single-Cell Demonstration: LangGraph Execution Timeline ======\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_classic.callbacks.tracers import LangChainTracer\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# 1. Define State\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    draft: str\n",
    "    reviewed: str\n",
    "\n",
    "# 2. Initialize Model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 3. Define Nodes\n",
    "def planner(state: State):\n",
    "    return {\"draft\": llm.invoke(f\"Draft answer: {state['question']}\").content}\n",
    "\n",
    "def reviewer(state: State):\n",
    "    return {\"reviewed\": llm.invoke(f\"Improve: {state['draft']}\").content}\n",
    "\n",
    "# 4. Build Graph\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"planner\", planner)\n",
    "builder.add_node(\"reviewer\", reviewer)\n",
    "\n",
    "builder.set_entry_point(\"planner\")\n",
    "builder.add_edge(\"planner\", \"reviewer\")\n",
    "builder.add_edge(\"reviewer\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# 5. Enable Tracing (Execution Timeline)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langgraph-timeline-demo\"\n",
    "\n",
    "tracer = LangChainTracer()\n",
    "\n",
    "# 6. Execute Graph\n",
    "result = graph.invoke(\n",
    "    {\"question\": \"Explain attention in transformers\"},\n",
    "    config={\"callbacks\": [tracer]}\n",
    ")\n",
    "\n",
    "# 7. Output Final Result\n",
    "print(\"\\n=== Final Output ===\\n\")\n",
    "print(result[\"reviewed\"])\n",
    "\n",
    "# 8. View Timeline\n",
    "print(\"\\nOpen LangSmith to view full Execution Timeline:\")\n",
    "print(\"https://smith.langchain.com\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
