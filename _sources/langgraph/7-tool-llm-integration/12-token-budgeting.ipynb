{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec00fb20-b336-4124-b0e4-dd0c7ed3657d",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## **Token Budgeting in LangGraph**\n",
    "\n",
    "**Token budgeting** is the discipline of **controlling, allocating, monitoring, and optimizing token usage** across an LLM workflow so that the system remains **cost-efficient, latency-bounded, stable, and scalable**.\n",
    "\n",
    "In LangGraph, token budgeting is a **first-class production concern** because graphs execute **multi-step, cyclic, multi-agent workflows** where uncontrolled token growth can cause **cost explosions, slowdowns, and failures**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Why Token Budgeting Is Critical in LangGraph**\n",
    "\n",
    "Unlike simple LLM calls, LangGraph workflows involve:\n",
    "\n",
    "* Loops (ReAct, reflection, self-healing)\n",
    "* Multi-agent collaboration\n",
    "* Long-running sessions\n",
    "* Persistent memory\n",
    "\n",
    "Without token control, these cause:\n",
    "\n",
    "| Failure Mode             | Effect                  |\n",
    "| ------------------------ | ----------------------- |\n",
    "| Unbounded context growth | Context window overflow |\n",
    "| High latency             | Slow responses          |\n",
    "| Cost explosion           | Budget violations       |\n",
    "| Agent instability        | Hallucinations & drift  |\n",
    "| Production outages       | Failed executions       |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Token Budgeting Objectives**\n",
    "\n",
    "| Objective            | Description                 |\n",
    "| -------------------- | --------------------------- |\n",
    "| Context safety       | Stay within model limits    |\n",
    "| Cost predictability  | Bound spending per run      |\n",
    "| Latency control      | Reduce prompt size          |\n",
    "| Quality preservation | Keep essential information  |\n",
    "| Scalability          | Serve many concurrent users |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Token Budget Architecture in LangGraph**\n",
    "\n",
    "```\n",
    "User Input\n",
    "   ↓\n",
    "State Memory (messages, tools, plans)\n",
    "   ↓\n",
    "Token Controller\n",
    "   ├─ Budget Allocation\n",
    "   ├─ Trimming Policy\n",
    "   ├─ Compression Policy\n",
    "   ├─ Routing Policy\n",
    "   └─ Fallback Policy\n",
    "   ↓\n",
    "LLM Node Execution\n",
    "```\n",
    "\n",
    "Token management occurs at **every LLM node invocation**.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Where Tokens Accumulate**\n",
    "\n",
    "| Source                      | Tokens   |\n",
    "| --------------------------- | -------- |\n",
    "| Conversation history        | High     |\n",
    "| Tool outputs                | Medium   |\n",
    "| Agent messages              | High     |\n",
    "| Plans / reflections         | Medium   |\n",
    "| Long-term memory retrievals | Variable |\n",
    "| System instructions         | Constant |\n",
    "\n",
    "LangGraph stores most of these inside the **shared state**.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Core Token Control Techniques**\n",
    "\n",
    "#### **A. Hard Token Limits**\n",
    "\n",
    "```python\n",
    "llm = ChatOpenAI(max_tokens=512)\n",
    "```\n",
    "\n",
    "Controls output length.\n",
    "\n",
    "---\n",
    "\n",
    "#### **B. State Trimming Policy**\n",
    "\n",
    "```python\n",
    "def trim_messages(state, max_tokens=2000):\n",
    "    while count_tokens(state[\"messages\"]) > max_tokens:\n",
    "        state[\"messages\"].pop(0)\n",
    "```\n",
    "\n",
    "Applied before LLM node execution.\n",
    "\n",
    "---\n",
    "\n",
    "#### **C. Sliding Window Memory**\n",
    "\n",
    "Keep only most recent context:\n",
    "\n",
    "```\n",
    "[System] + [Last N messages] + [Current task]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **D. Semantic Compression**\n",
    "\n",
    "Summarize older context:\n",
    "\n",
    "```python\n",
    "def compress(state):\n",
    "    summary = summarize(state[\"messages\"])\n",
    "    state[\"messages\"] = [summary]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **E. Budget Allocation Per Node**\n",
    "\n",
    "| Node         | Token Budget |\n",
    "| ------------ | ------------ |\n",
    "| Planner      | 1000         |\n",
    "| Executor     | 800          |\n",
    "| Reflection   | 500          |\n",
    "| Tool Summary | 300          |\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Token-Aware Router Example**\n",
    "\n",
    "```python\n",
    "def route(state):\n",
    "    if count_tokens(state[\"messages\"]) > 3000:\n",
    "        return \"compress\"\n",
    "    return \"reason\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Token Budgeting in Cyclic Graphs**\n",
    "\n",
    "In loops, tokens accumulate rapidly.\n",
    "LangGraph requires **loop-level token guards**:\n",
    "\n",
    "```python\n",
    "config = {\"recursion_limit\": 10}\n",
    "```\n",
    "\n",
    "And per-cycle trimming:\n",
    "\n",
    "```python\n",
    "def loop_guard(state):\n",
    "    state[\"messages\"] = trim_to_budget(state[\"messages\"], 2000)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Production Token Budgeting Strategy**\n",
    "\n",
    "| Layer    | Strategy                      |\n",
    "| -------- | ----------------------------- |\n",
    "| LLM Call | Hard max_tokens               |\n",
    "| State    | Sliding window + compression  |\n",
    "| Loop     | Token guard + recursion limit |\n",
    "| Agent    | Role-based token quotas       |\n",
    "| System   | Cost monitoring               |\n",
    "| Fallback | Smaller model routing         |\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Token Budgeting Variants**\n",
    "\n",
    "| Variant          | Use Case              |\n",
    "| ---------------- | --------------------- |\n",
    "| Strict Budget    | Finance / enterprise  |\n",
    "| Soft Budget      | Creative apps         |\n",
    "| Adaptive Budget  | Load-aware systems    |\n",
    "| Per-Agent Budget | Multi-agent platforms |\n",
    "| Session Budget   | SaaS cost control     |\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Monitoring & Enforcement**\n",
    "\n",
    "| Metric         | Purpose           |\n",
    "| -------------- | ----------------- |\n",
    "| Tokens / run   | Cost              |\n",
    "| Tokens / user  | Quotas            |\n",
    "| Tokens / agent | Optimization      |\n",
    "| Tokens / node  | Hotspot detection |\n",
    "| Latency        | Performance       |\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Mental Model**\n",
    "\n",
    "> **Tokens are the currency of LLM systems.\n",
    "> LangGraph is the central bank.**\n",
    "\n",
    "Every node spends from the shared token economy.\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff02c97e-b8b6-4e0a-89ab-d30941c1181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Steps: 3\n",
      "Final Token Count: 96\n",
      "\n",
      "Final Messages:\n",
      "\n",
      "AI : LangGraph token budgeting involves managing the allocation of tokens consumed by large language models (LLMs) to enhance performance, cost-efficiency, and overall effectiveness in various applications\n",
      "AI : allocated optimally to maintain high-quality outputs while minimizing costs. Here are some key aspects to consider for token budgeting in LangGraph or similar scenarios:\n",
      "\n",
      "1. **Understanding Token Limi\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.schema import BaseMessage, HumanMessage\n",
    "\n",
    "# ----------------------------\n",
    "# State\n",
    "# ----------------------------\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    steps: int\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "\n",
    "def count_tokens(messages):\n",
    "    return sum(len(m.content.split()) for m in messages)\n",
    "\n",
    "def trim_to_budget(messages, budget=120):\n",
    "    while count_tokens(messages) > budget:\n",
    "        messages.pop(0)\n",
    "    return messages\n",
    "\n",
    "# ----------------------------\n",
    "# LLM\n",
    "# ----------------------------\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=60)\n",
    "\n",
    "# ----------------------------\n",
    "# Nodes\n",
    "# ----------------------------\n",
    "\n",
    "def reason(state: State):\n",
    "    state[\"messages\"] = trim_to_budget(state[\"messages\"], 120)\n",
    "    reply = llm.invoke(state[\"messages\"])\n",
    "    state[\"messages\"].append(reply)\n",
    "    state[\"steps\"] += 1\n",
    "    return state\n",
    "\n",
    "def compress(state: State):\n",
    "    summary = llm.invoke([HumanMessage(content=f\"Summarize briefly: {state['messages']}\")])\n",
    "    state[\"messages\"] = [summary]\n",
    "    return state\n",
    "\n",
    "def router(state: State):\n",
    "    if state[\"steps\"] >= 3:\n",
    "        return END\n",
    "    if count_tokens(state[\"messages\"]) > 100:\n",
    "        return \"compress\"\n",
    "    return \"reason\"\n",
    "\n",
    "# ----------------------------\n",
    "# Graph\n",
    "# ----------------------------\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"reason\", reason)\n",
    "builder.add_node(\"compress\", compress)\n",
    "\n",
    "builder.set_entry_point(\"reason\")\n",
    "\n",
    "builder.add_conditional_edges(\"reason\", router, {\n",
    "    \"compress\": \"compress\",\n",
    "    \"reason\": \"reason\",\n",
    "    END: END\n",
    "})\n",
    "\n",
    "builder.add_edge(\"compress\", \"reason\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# ----------------------------\n",
    "# Run\n",
    "# ----------------------------\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=\"Explain LangGraph token budgeting with examples.\")],\n",
    "    \"steps\": 0\n",
    "}\n",
    "\n",
    "result = graph.invoke(initial_state, config={\"recursion_limit\": 10})\n",
    "\n",
    "print(\"Final Steps:\", result[\"steps\"])\n",
    "print(\"Final Token Count:\", count_tokens(result[\"messages\"]))\n",
    "print(\"\\nFinal Messages:\\n\")\n",
    "\n",
    "for m in result[\"messages\"]:\n",
    "    print(m.type.upper(), \":\", m.content[:200])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
