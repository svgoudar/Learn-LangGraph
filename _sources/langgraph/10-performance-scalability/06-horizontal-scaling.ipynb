{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0102fc6",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Horizontal Scaling\n",
    "\n",
    "**Horizontal scaling** in LangGraph refers to increasing system throughput and reliability by running **multiple concurrent LangGraph runtimes** across machines or containers while sharing execution state, memory, and coordination services.\n",
    "\n",
    "It enables LangGraph systems to support **high traffic, multi-tenant workloads, and long-running autonomous agents**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Why Horizontal Scaling Matters for LangGraph**\n",
    "\n",
    "LangGraph workloads exhibit:\n",
    "\n",
    "* High concurrency (many users, many agents)\n",
    "* Long-running workflows\n",
    "* Stateful execution with checkpoints\n",
    "* Heavy LLM + tool calls\n",
    "\n",
    "Vertical scaling (bigger machine) fails quickly under such loads.\n",
    "LangGraph therefore relies on **distributed execution**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Core Architecture Model**\n",
    "\n",
    "```\n",
    "Clients\n",
    "   |\n",
    "Load Balancer\n",
    "   |\n",
    "API Gateway\n",
    "   |\n",
    "Multiple LangGraph Runtimes  (Pods / VMs)\n",
    "   |\n",
    "Shared Infrastructure\n",
    "   ├── State Store (Redis / Postgres)\n",
    "   ├── Checkpoint Store\n",
    "   ├── Vector Store\n",
    "   └── Message Queue\n",
    "```\n",
    "\n",
    "Each LangGraph runtime is **stateless at the compute layer** and **stateful through shared stores**.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. What Gets Scaled**\n",
    "\n",
    "| Component         | Scaling Strategy |\n",
    "| ----------------- | ---------------- |\n",
    "| LangGraph Runtime | Horizontal       |\n",
    "| LLM Gateways      | Horizontal       |\n",
    "| Tool Workers      | Horizontal       |\n",
    "| Agent Pools       | Horizontal       |\n",
    "| Schedulers        | Horizontal       |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Key Production Components**\n",
    "\n",
    "| Layer            | Technology          |\n",
    "| ---------------- | ------------------- |\n",
    "| Load Balancer    | NGINX / ALB         |\n",
    "| Runtime          | FastAPI + LangGraph |\n",
    "| State Store      | Redis / PostgreSQL  |\n",
    "| Checkpoint Store | S3 / GCS            |\n",
    "| Message Queue    | Kafka / RabbitMQ    |\n",
    "| Vector DB        | Pinecone / Weaviate |\n",
    "| Orchestration    | Kubernetes          |\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Execution Model Under Horizontal Scaling**\n",
    "\n",
    "Each request:\n",
    "\n",
    "1. Hits any available LangGraph pod\n",
    "2. Loads graph + state from shared store\n",
    "3. Executes next step(s)\n",
    "4. Writes updated state back\n",
    "5. Can resume on **any** pod\n",
    "\n",
    "This allows:\n",
    "\n",
    "* Failover\n",
    "* Migration\n",
    "* Elastic scaling\n",
    "* Zero-downtime upgrades\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Concurrency Control**\n",
    "\n",
    "LangGraph uses:\n",
    "\n",
    "* **Thread IDs** → workflow identity\n",
    "* **Optimistic locking** on state writes\n",
    "* **Versioned checkpoints**\n",
    "* **Idempotent node execution**\n",
    "\n",
    "Preventing:\n",
    "\n",
    "* Duplicate execution\n",
    "* Race conditions\n",
    "* Lost updates\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Scaling Example (FastAPI)**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/run\")\n",
    "async def run_graph(input: dict):\n",
    "    result = graph.invoke(input)\n",
    "    return result\n",
    "```\n",
    "\n",
    "Deploy with Kubernetes:\n",
    "\n",
    "```bash\n",
    "kubectl scale deployment langgraph-runtime --replicas=10\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Autoscaling Policies**\n",
    "\n",
    "| Metric        | Action   |\n",
    "| ------------- | -------- |\n",
    "| Request rate  | Add pods |\n",
    "| CPU usage     | Add pods |\n",
    "| LLM latency   | Add pods |\n",
    "| Queue backlog | Add pods |\n",
    "\n",
    "Kubernetes HPA:\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Fault Tolerance Guarantees**\n",
    "\n",
    "| Failure           | Behavior                 |\n",
    "| ----------------- | ------------------------ |\n",
    "| Pod crash         | Resume from checkpoint   |\n",
    "| Node failure      | Reassigned automatically |\n",
    "| LLM timeout       | Retry logic              |\n",
    "| Partial execution | No corruption            |\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Horizontal vs Vertical Scaling**\n",
    "\n",
    "| Feature           | Horizontal | Vertical |\n",
    "| ----------------- | ---------- | -------- |\n",
    "| Capacity          | Unlimited  | Limited  |\n",
    "| Resilience        | High       | Low      |\n",
    "| Cost efficiency   | High       | Low      |\n",
    "| Failure isolation | Strong     | Weak     |\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Real-World Use Cases**\n",
    "\n",
    "* Enterprise AI assistants\n",
    "* Multi-agent research systems\n",
    "* Autonomous monitoring platforms\n",
    "* Large-scale customer support bots\n",
    "\n",
    "---\n",
    "\n",
    "### **12. Mental Model**\n",
    "\n",
    "LangGraph becomes a **distributed state machine** where:\n",
    "\n",
    "> **Compute is disposable, state is persistent.**\n",
    "\n",
    "This is the fundamental principle enabling safe horizontal scaling.\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd85fd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 1 executed step 1\n",
      "Worker 1 executed step 2\n",
      "Worker 2 executed step 3\n",
      "Worker 2 executed step 4\n",
      "Worker 3 executed step 5\n",
      "\n",
      "Final State: {'thread_id': 'job-123', 'step': 5, 'done': True}\n"
     ]
    }
   ],
   "source": [
    "# One-cell demonstration: Horizontally-scaled LangGraph execution\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "import random\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Shared Persistent State (simulated DB)\n",
    "# ----------------------------\n",
    "GLOBAL_STATE_STORE = {}\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Graph State Definition\n",
    "# ----------------------------\n",
    "class State(TypedDict):\n",
    "    thread_id: str\n",
    "    step: int\n",
    "    done: bool\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Nodes\n",
    "# ----------------------------\n",
    "def work(state: State) -> State:\n",
    "    state[\"step\"] += 1\n",
    "    if state[\"step\"] >= 5:\n",
    "        state[\"done\"] = True\n",
    "    print(f\"Worker {random.randint(1,3)} executed step {state['step']}\")\n",
    "    return state\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Routing Logic\n",
    "# ----------------------------\n",
    "def router(state: State):\n",
    "    return END if state[\"done\"] else \"work\"\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Graph Construction\n",
    "# ----------------------------\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"work\", work)\n",
    "builder.set_entry_point(\"work\")\n",
    "builder.add_conditional_edges(\"work\", router, {\"work\": \"work\", END: END})\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Simulated Distributed Execution\n",
    "# ----------------------------\n",
    "thread_id = \"job-123\"\n",
    "\n",
    "# Load from shared store (like Redis/Postgres)\n",
    "state = GLOBAL_STATE_STORE.get(thread_id, {\"thread_id\": thread_id, \"step\": 0, \"done\": False})\n",
    "\n",
    "while not state[\"done\"]:\n",
    "    state = graph.invoke(state)\n",
    "    # Save back to shared store (like DB)\n",
    "    GLOBAL_STATE_STORE[thread_id] = state\n",
    "\n",
    "print(\"\\nFinal State:\", GLOBAL_STATE_STORE[thread_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb43da34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
