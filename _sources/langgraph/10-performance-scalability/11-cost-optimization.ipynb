{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ca0c23-b7e0-419d-b7ca-6fe0f781f5d8",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Cost Optimization \n",
    "\n",
    "**Cost optimization in LangGraph** is the systematic design of agent workflows, execution graphs, and infrastructure to **minimize LLM, tool, memory, and compute costs** while preserving correctness, reliability, and performance.\n",
    "\n",
    "In production systems, **90%+ of operational cost comes from model calls and token usage**, making cost optimization a **first-class architectural concern**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Where Costs Originate in LangGraph**\n",
    "\n",
    "| Cost Source        | Description                 |\n",
    "| ------------------ | --------------------------- |\n",
    "| LLM Inference      | Prompt + completion tokens  |\n",
    "| Tool Calls         | External API usage          |\n",
    "| Vector Search      | Embeddings + queries        |\n",
    "| State Storage      | Checkpoints, memory, logs   |\n",
    "| Execution Overhead | Retries, loops, concurrency |\n",
    "| Infrastructure     | CPU, RAM, networking        |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Core Cost-Optimization Principles**\n",
    "\n",
    "| Principle               | Purpose                 |\n",
    "| ----------------------- | ----------------------- |\n",
    "| Minimize Tokens         | Reduce LLM spend        |\n",
    "| Reduce Model Calls      | Fewer invocations       |\n",
    "| Use Cheaper Models      | Route intelligently     |\n",
    "| Shorten Execution Paths | Less runtime            |\n",
    "| Cache Aggressively      | Avoid recomputation     |\n",
    "| Terminate Early         | Avoid unnecessary loops |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Token-Level Optimization**\n",
    "\n",
    "#### **Prompt Compression**\n",
    "\n",
    "```python\n",
    "def compact_prompt(state):\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"][-3:]  # keep only last 3 turns\n",
    "    }\n",
    "```\n",
    "\n",
    "#### **Structured Output**\n",
    "\n",
    "```python\n",
    "system_prompt = \"Respond in strict JSON. No explanations.\"\n",
    "```\n",
    "\n",
    "Reduces verbosity → **30–50% token savings**.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Model Routing Strategy**\n",
    "\n",
    "```python\n",
    "def choose_model(state):\n",
    "    if state[\"complexity\"] < 0.3:\n",
    "        return \"gpt-3.5\"\n",
    "    return \"gpt-4\"\n",
    "```\n",
    "\n",
    "| Task Type            | Model         |\n",
    "| -------------------- | ------------- |\n",
    "| Simple extraction    | Cheap model   |\n",
    "| Reasoning / planning | Premium model |\n",
    "| Verification         | Cheap model   |\n",
    "\n",
    "**Savings:** up to **70%** on inference cost.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Graph-Level Optimization**\n",
    "\n",
    "#### **Early Exit Conditions**\n",
    "\n",
    "```python\n",
    "def router(state):\n",
    "    if state[\"confidence\"] > 0.9:\n",
    "        return END\n",
    "    return \"refine\"\n",
    "```\n",
    "\n",
    "#### **Cycle Budgeting**\n",
    "\n",
    "```python\n",
    "graph.invoke(input, config={\"recursion_limit\": 5})\n",
    "```\n",
    "\n",
    "Prevents runaway loops.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Caching & Memoization**\n",
    "\n",
    "```python\n",
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "```\n",
    "\n",
    "| Cache Layer     | Savings                   |\n",
    "| --------------- | ------------------------- |\n",
    "| Prompt cache    | Skip repeated calls       |\n",
    "| Tool cache      | Avoid duplicate API calls |\n",
    "| Embedding cache | Eliminate recomputation   |\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Parallelization & Batching**\n",
    "\n",
    "```python\n",
    "builder.add_node(\"parallel_eval\", async_node)\n",
    "```\n",
    "\n",
    "Batch LLM requests:\n",
    "\n",
    "| Benefit       | Impact                |\n",
    "| ------------- | --------------------- |\n",
    "| Lower latency | Faster response       |\n",
    "| Lower cost    | Fewer overhead tokens |\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Checkpointing & Recovery**\n",
    "\n",
    "Avoid re-running expensive steps:\n",
    "\n",
    "```python\n",
    "graph = builder.compile(checkpointer=PostgresSaver())\n",
    "```\n",
    "\n",
    "If crash → resume from checkpoint.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Cost Observability**\n",
    "\n",
    "| Tool             | Role                  |\n",
    "| ---------------- | --------------------- |\n",
    "| Token meters     | Measure usage         |\n",
    "| Per-run budgets  | Hard caps             |\n",
    "| Alert thresholds | Prevent overruns      |\n",
    "| Usage dashboards | Optimization feedback |\n",
    "\n",
    "```python\n",
    "graph.invoke(input, config={\"max_cost\": 0.02})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Enterprise Cost Control Architecture**\n",
    "\n",
    "| Layer        | Optimization           |\n",
    "| ------------ | ---------------------- |\n",
    "| Prompt layer | Compression, structure |\n",
    "| Graph layer  | Short paths, pruning   |\n",
    "| Agent layer  | Smart routing          |\n",
    "| Model layer  | Tiered models          |\n",
    "| Memory layer | TTL, eviction          |\n",
    "| Ops layer    | Autoscaling            |\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Measurable Impact (Real Systems)**\n",
    "\n",
    "| Optimization        | Cost Reduction |\n",
    "| ------------------- | -------------- |\n",
    "| Prompt compression  | 20–40%         |\n",
    "| Model routing       | 40–70%         |\n",
    "| Caching             | 50–90%         |\n",
    "| Loop control        | 30–60%         |\n",
    "| Checkpoint recovery | 20–50%         |\n",
    "\n",
    "---\n",
    "\n",
    "### **12. Mental Model**\n",
    "\n",
    "> **Every unnecessary token is money.\n",
    "> Every unnecessary node is cost.\n",
    "> Every unnecessary loop is burn.**\n",
    "\n",
    "Cost optimization in LangGraph is therefore **graph design + agent policy + runtime governance**.\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdac122e-191c-4a6c-80e9-df6ac1d22811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Explain black holes simply', 'complexity': 0.6, 'answer': '{\\'answer\\': \"Black holes are regions in space where gravity is so strong that nothing, not even light, can escape from them. They are formed when a large star collapses under its own gravity after its life cycle ends. Inside a black hole, all the matter is squeezed into a tiny space, creating a point of infinite density called a singularity.\"}', 'confidence': 0.95}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.cache import InMemoryCache\n",
    "import langchain\n",
    "\n",
    "# --------- Global Cache ---------\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "# --------- State ---------\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    complexity: float\n",
    "    answer: str\n",
    "    confidence: float\n",
    "\n",
    "# --------- Models ---------\n",
    "cheap_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "strong_model = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "# --------- Cost-Aware Router ---------\n",
    "def route_model(state: State):\n",
    "    if state[\"complexity\"] < 0.4:\n",
    "        return \"cheap\"\n",
    "    return \"strong\"\n",
    "\n",
    "# --------- Nodes ---------\n",
    "def solve_with_cheap(state):\n",
    "    msg = f\"Answer briefly: {state['query']}\"\n",
    "    resp = cheap_model.invoke(msg)\n",
    "    return {\"answer\": resp.content, \"confidence\": 0.7}\n",
    "\n",
    "def solve_with_strong(state):\n",
    "    msg = f\"Answer concisely with JSON: {{'answer': ...}}. Question: {state['query']}\"\n",
    "    resp = strong_model.invoke(msg)\n",
    "    return {\"answer\": resp.content, \"confidence\": 0.95}\n",
    "\n",
    "def should_stop(state):\n",
    "    if state[\"confidence\"] > 0.9:\n",
    "        return END\n",
    "    return \"strong\"\n",
    "\n",
    "# --------- Graph ---------\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"cheap\", solve_with_cheap)\n",
    "builder.add_node(\"strong\", solve_with_strong)\n",
    "\n",
    "builder.set_entry_point(\"cheap\")\n",
    "\n",
    "builder.add_conditional_edges(\"cheap\", route_model, {\n",
    "    \"cheap\": END,\n",
    "    \"strong\": \"strong\"\n",
    "})\n",
    "\n",
    "builder.add_conditional_edges(\"strong\", should_stop, {\n",
    "    \"strong\": \"strong\",\n",
    "    END: END\n",
    "})\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# --------- Run with Loop & Cost Control ---------\n",
    "result = graph.invoke(\n",
    "    {\"query\": \"Explain black holes simply\", \"complexity\": 0.6},\n",
    "    config={\"recursion_limit\": 3}\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
