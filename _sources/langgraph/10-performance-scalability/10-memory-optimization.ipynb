{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf522853-5180-41d7-999f-68b4730c9bd4",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Latency Optimization \n",
    "\n",
    "**Latency optimization** in LangGraph is the systematic process of **minimizing end-to-end response time** of an LLM workflow by optimizing graph structure, execution flow, model usage, tool invocation, memory access, and infrastructure behavior — while preserving correctness and reliability.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Where Latency Comes From in LangGraph**\n",
    "\n",
    "| Source           | Description                 |\n",
    "| ---------------- | --------------------------- |\n",
    "| LLM Inference    | Model compute + network     |\n",
    "| Tool Calls       | External APIs, databases    |\n",
    "| Graph Overhead   | Scheduling & routing        |\n",
    "| State Operations | Serialization, persistence  |\n",
    "| Memory Access    | Vector search, DB IO        |\n",
    "| Network          | Cross-service communication |\n",
    "| Retries          | Failure recovery delays     |\n",
    "\n",
    "Total latency is the **sum of all node execution times + orchestration overhead**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Latency Optimization Strategy Map**\n",
    "\n",
    "```\n",
    "Graph Design\n",
    "   ↓\n",
    "Execution Model\n",
    "   ↓\n",
    "Model & Prompt Design\n",
    "   ↓\n",
    "Tool & Memory Optimization\n",
    "   ↓\n",
    "Infrastructure Optimization\n",
    "```\n",
    "\n",
    "Each layer must be optimized.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Graph-Level Optimizations**\n",
    "\n",
    "### **3.1 Reduce Critical Path Length**\n",
    "\n",
    "Minimize sequential nodes on the main execution path.\n",
    "\n",
    "❌ Slow:\n",
    "\n",
    "```\n",
    "A → B → C → D → E\n",
    "```\n",
    "\n",
    "✅ Faster:\n",
    "\n",
    "```\n",
    "A → (B || C) → D → E\n",
    "```\n",
    "\n",
    "```python\n",
    "builder.add_edge(\"A\", [\"B\", \"C\"])   # fan-out\n",
    "builder.add_edge([\"B\", \"C\"], \"D\")   # join\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3.2 Parallel Execution**\n",
    "\n",
    "Execute independent tasks concurrently.\n",
    "\n",
    "Use **async nodes** for non-dependent operations.\n",
    "\n",
    "```python\n",
    "async def tool_a(state): ...\n",
    "async def tool_b(state): ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3.3 Early Exit Routing**\n",
    "\n",
    "Terminate execution as soon as result is available.\n",
    "\n",
    "```python\n",
    "def router(state):\n",
    "    if state[\"confidence\"] > 0.95:\n",
    "        return END\n",
    "    return \"verify\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3.4 Limit Cycles**\n",
    "\n",
    "Every loop multiplies latency.\n",
    "\n",
    "```python\n",
    "graph.invoke(data, config={\"recursion_limit\": 5})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Model-Level Optimizations**\n",
    "\n",
    "### **4.1 Right-Size the Model**\n",
    "\n",
    "| Task       | Model        |\n",
    "| ---------- | ------------ |\n",
    "| Routing    | Small / fast |\n",
    "| Extraction | Medium       |\n",
    "| Reasoning  | Large        |\n",
    "\n",
    "```python\n",
    "router_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "reason_llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4.2 Prompt Compression**\n",
    "\n",
    "* Remove redundancy\n",
    "* Use structured prompts\n",
    "* Limit context window\n",
    "\n",
    "---\n",
    "\n",
    "### **4.3 Streaming**\n",
    "\n",
    "Reduce **perceived latency** by returning tokens immediately.\n",
    "\n",
    "```python\n",
    "graph.stream(input)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Tool & Memory Optimization**\n",
    "\n",
    "### **5.1 Cache Everything**\n",
    "\n",
    "| Layer         | Cache           |\n",
    "| ------------- | --------------- |\n",
    "| LLM responses | Redis           |\n",
    "| Tool calls    | HTTP cache      |\n",
    "| Vector search | Embedding cache |\n",
    "| State         | In-memory store |\n",
    "\n",
    "---\n",
    "\n",
    "### **5.2 Batch Operations**\n",
    "\n",
    "Batch tool calls and vector queries.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.3 Lazy Memory Loading**\n",
    "\n",
    "Load memory only when required.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. State & Persistence Optimization**\n",
    "\n",
    "* Avoid large state objects\n",
    "* Store references, not raw blobs\n",
    "* Reduce checkpoint frequency\n",
    "* Use in-memory store for hot paths\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Infrastructure Optimizations**\n",
    "\n",
    "| Component   | Optimization       |\n",
    "| ----------- | ------------------ |\n",
    "| LLM API     | Regional routing   |\n",
    "| Network     | Co-locate services |\n",
    "| Compute     | Warm containers    |\n",
    "| Storage     | SSD-backed stores  |\n",
    "| Scaling     | Auto-scale workers |\n",
    "| Concurrency | Async runtime      |\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Measuring & Enforcing Latency**\n",
    "\n",
    "```python\n",
    "graph.invoke(data, config={\n",
    "    \"timeout\": 8,\n",
    "    \"recursion_limit\": 4\n",
    "})\n",
    "```\n",
    "\n",
    "Monitor:\n",
    "\n",
    "* P50 / P90 / P99 latency\n",
    "* Node execution times\n",
    "* Queue depth\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Production Latency Playbook**\n",
    "\n",
    "| Symptom           | Fix                   |\n",
    "| ----------------- | --------------------- |\n",
    "| Slow responses    | Reduce model size     |\n",
    "| Long tail latency | Parallelize           |\n",
    "| Spikes            | Add caching           |\n",
    "| Timeouts          | Early exit            |\n",
    "| High cost         | Prompt & model tuning |\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Final Principle**\n",
    "\n",
    "> **Optimize the graph before optimizing the model.\n",
    "> Most latency problems are orchestration problems.**\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8be9f75-5cc1-4eb8-8075-96e51ea2149b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: {'query': 'simple question', 'confidence': 0.99, 'answer': 'Quick answer.'}\n",
      "Latency: 0.15 seconds\n"
     ]
    }
   ],
   "source": [
    "# ======== Latency-Optimized LangGraph Demo (Correct) ========\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# ---- State ----\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    confidence: float\n",
    "    answer: str\n",
    "\n",
    "# ---- Fast & Slow models (simulated) ----\n",
    "async def fast_router(state: State):\n",
    "    await asyncio.sleep(0.1)\n",
    "    if \"simple\" in state[\"query\"]:\n",
    "        return {\"confidence\": 0.99, \"answer\": \"Quick answer.\"}\n",
    "    return {\"confidence\": 0.4}\n",
    "\n",
    "async def slow_reasoner(state: State):\n",
    "    await asyncio.sleep(1.5)\n",
    "    return {\"answer\": f\"Deep reasoning for: {state['query']}\", \"confidence\": 0.96}\n",
    "\n",
    "# ---- Router logic (sync) ----\n",
    "def route(state: State):\n",
    "    if state[\"confidence\"] > 0.95:\n",
    "        return END\n",
    "    return \"reason\"\n",
    "\n",
    "# ---- Graph ----\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"router\", fast_router)\n",
    "builder.add_node(\"reason\", slow_reasoner)\n",
    "\n",
    "builder.set_entry_point(\"router\")\n",
    "builder.add_conditional_edges(\"router\", route, {\n",
    "    \"reason\": \"reason\",\n",
    "    END: END\n",
    "})\n",
    "builder.add_edge(\"reason\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# ---- Run correctly with async runtime ----\n",
    "async def run():\n",
    "    start = time.time()\n",
    "    result = await graph.ainvoke({\n",
    "        \"query\": \"simple question\",\n",
    "        \"confidence\": 0.0,\n",
    "        \"answer\": \"\"\n",
    "    }, config={\"timeout\": 5, \"recursion_limit\": 3})\n",
    "    print(\"Result:\", result)\n",
    "    print(\"Latency:\", round(time.time() - start, 2), \"seconds\")\n",
    "\n",
    "await run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
