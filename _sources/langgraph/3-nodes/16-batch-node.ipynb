{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b16a88",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## **Batch Node in LangGraph**\n",
    "\n",
    "A **Batch Node** in LangGraph is a specialized execution pattern that allows a single node to **process multiple independent inputs concurrently** within one graph step. It is essential for **high-throughput, low-latency, cost-efficient production systems** such as document processing pipelines, data labeling systems, and large-scale inference workflows.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Motivation**\n",
    "\n",
    "Without batching, each item is processed sequentially:\n",
    "\n",
    "```\n",
    "item1 → node → result1  \n",
    "item2 → node → result2  \n",
    "item3 → node → result3\n",
    "```\n",
    "\n",
    "With a Batch Node:\n",
    "\n",
    "```\n",
    "[item1, item2, item3] → Batch Node → [result1, result2, result3]\n",
    "```\n",
    "\n",
    "This improves:\n",
    "\n",
    "| Metric          | Improvement       |\n",
    "| --------------- | ----------------- |\n",
    "| Throughput      | High              |\n",
    "| Latency         | Lower             |\n",
    "| Cost            | Reduced LLM calls |\n",
    "| GPU utilization | Better            |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Conceptual Model**\n",
    "\n",
    "A Batch Node is still a **single node**, but its internal function consumes and produces **lists of state fragments**.\n",
    "\n",
    "```\n",
    "Graph Step\n",
    "   └── Batch Node\n",
    "           ├─ Task 1\n",
    "           ├─ Task 2\n",
    "           ├─ Task 3\n",
    "           └─ Task N\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. State Design for Batching**\n",
    "\n",
    "The state must explicitly support collections.\n",
    "\n",
    "```python\n",
    "class State(TypedDict):\n",
    "    items: list[str]\n",
    "    results: list[str]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Implementing a Batch Node**\n",
    "\n",
    "```python\n",
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "class State(TypedDict):\n",
    "    items: List[str]\n",
    "    results: List[str]\n",
    "\n",
    "def batch_processor(state: State):\n",
    "    outputs = []\n",
    "    for text in state[\"items\"]:\n",
    "        outputs.append(text.upper())\n",
    "    return {\"results\": outputs}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"batch\", batch_processor)\n",
    "builder.set_entry_point(\"batch\")\n",
    "builder.add_edge(\"batch\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "result = graph.invoke({\n",
    "    \"items\": [\"doc one\", \"doc two\", \"doc three\"]\n",
    "})\n",
    "print(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Parallel Batch Execution (High Performance)**\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def async_batch_processor(state):\n",
    "    async def process(item):\n",
    "        return item.upper()\n",
    "\n",
    "    tasks = [process(x) for x in state[\"items\"]]\n",
    "    outputs = await asyncio.gather(*tasks)\n",
    "    return {\"results\": outputs}\n",
    "```\n",
    "\n",
    "Use:\n",
    "\n",
    "```python\n",
    "builder.add_node(\"batch\", async_batch_processor)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. LLM Batch Node Example**\n",
    "\n",
    "```python\n",
    "def summarize_batch(state):\n",
    "    summaries = llm.batch(state[\"items\"])\n",
    "    return {\"results\": summaries}\n",
    "```\n",
    "\n",
    "This reduces **N LLM calls → 1 batched LLM call**.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. When to Use Batch Nodes**\n",
    "\n",
    "| Use Case               | Benefit         |\n",
    "| ---------------------- | --------------- |\n",
    "| Document summarization | Massive speedup |\n",
    "| Embedding generation   | Cost reduction  |\n",
    "| Data labeling          | Throughput      |\n",
    "| Evaluation pipelines   | Scalability     |\n",
    "| ETL workflows          | Efficiency      |\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Production Concerns**\n",
    "\n",
    "| Concern         | Handling                |\n",
    "| --------------- | ----------------------- |\n",
    "| Memory pressure | Chunk batches           |\n",
    "| Error isolation | Per-item try/catch      |\n",
    "| Retry           | Retry failed items only |\n",
    "| Observability   | Log per item            |\n",
    "| Backpressure    | Limit batch size        |\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Variants of Batch Nodes**\n",
    "\n",
    "| Variant            | Description                 |\n",
    "| ------------------ | --------------------------- |\n",
    "| Static Batch       | Fixed-size input            |\n",
    "| Dynamic Batch      | Accumulates until threshold |\n",
    "| Streaming Batch    | Processes rolling windows   |\n",
    "| Micro-Batch        | Low-latency mini batches    |\n",
    "| Hierarchical Batch | Batch inside subgraphs      |\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Mental Model**\n",
    "\n",
    "A Batch Node turns LangGraph from a **control system** into a **data-processing engine**, enabling it to behave like:\n",
    "\n",
    "> **Distributed data pipeline + intelligent orchestrator**\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4cef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class State(TypedDict):\n",
    "    documents: List[str]\n",
    "    summaries: List[str]\n",
    "\n",
    "def batch_summarizer(state: State):\n",
    "    results = []\n",
    "    for doc in state[\"documents\"]:\n",
    "        # mock LLM call\n",
    "        results.append(f\"Summary: {doc[:40]}...\")\n",
    "    return {\"summaries\": results}\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def async_batch_summarizer(state: State):\n",
    "    async def summarize(text):\n",
    "        await asyncio.sleep(0.1)  # simulate LLM latency\n",
    "        return f\"Summary: {text[:40]}...\"\n",
    "\n",
    "    tasks = [summarize(d) for d in state[\"documents\"]]\n",
    "    outputs = await asyncio.gather(*tasks)\n",
    "    return {\"summaries\": outputs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef237fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"batch\", async_batch_summarizer)\n",
    "builder.set_entry_point(\"batch\")\n",
    "builder.add_edge(\"batch\", END)\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13dd4d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: LangGraph enables cyclic workflows for L...', 'Summary: Batch nodes improve throughput and reduc...', 'Summary: Production graphs require fault toleranc...']\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "input_data = {\n",
    "    \"documents\": [\n",
    "        \"LangGraph enables cyclic workflows for LLM systems.\",\n",
    "        \"Batch nodes improve throughput and reduce cost.\",\n",
    "        \"Production graphs require fault tolerance.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "import asyncio\n",
    "\n",
    "result = await graph.ainvoke(input_data)\n",
    "print(result[\"summaries\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
