{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6ac0e8-1f02-4284-b5c9-a27951be86ce",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Prompt Template\n",
    "\n",
    "A **Prompt Template** in LangGraph is a **parameterized, reusable specification of how inputs, memory, and instructions are assembled into a final prompt** before being sent to a Large Language Model (LLM).\n",
    "It is the primary mechanism for **controlling model behavior**, **injecting state**, and **enforcing task structure** inside a graph-based workflow.\n",
    "\n",
    "LangGraph itself does not invent a new prompt system; it **builds on LangChain’s prompt abstraction**, but integrates it into a **state-driven execution model**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Why Prompt Templates Are Necessary**\n",
    "\n",
    "LLMs respond to **strings**.\n",
    "Applications operate on **structured data and evolving state**.\n",
    "\n",
    "Prompt templates serve as the **bridge**.\n",
    "\n",
    "| Problem              | Without Templates | With Templates      |\n",
    "| -------------------- | ----------------- | ------------------- |\n",
    "| Instruction drift    | Unstable outputs  | Controlled behavior |\n",
    "| Manual formatting    | Error-prone       | Automatic           |\n",
    "| State injection      | Ad-hoc            | Systematic          |\n",
    "| Multi-step workflows | Fragile           | Deterministic       |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Position in LangGraph Architecture**\n",
    "\n",
    "```\n",
    "Graph State ──► Prompt Template ──► LLM ──► Parsed Output ──► State Update\n",
    "```\n",
    "\n",
    "The template is evaluated **at runtime** using the current graph state.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Core Structure of a Prompt Template**\n",
    "\n",
    "A prompt template is composed of:\n",
    "\n",
    "1. **Static instructions**\n",
    "2. **Input variables**\n",
    "3. **Dynamic state injection**\n",
    "4. **Optional output constraints**\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "You are a technical assistant.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer clearly and concisely.\n",
    "\"\"\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Using Prompt Templates Inside a LangGraph Node**\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "def reasoning_node(state):\n",
    "    prompt = template.format(\n",
    "        question=state[\"question\"],\n",
    "        context=state[\"memory\"]\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"answer\": response.content}\n",
    "```\n",
    "\n",
    "This node becomes part of the graph execution.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Prompt Templates + State Binding**\n",
    "\n",
    "LangGraph encourages **explicit state → prompt binding**.\n",
    "\n",
    "```python\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    memory: str\n",
    "    answer: str\n",
    "```\n",
    "\n",
    "Each time the node runs, the template is **re-rendered** with the latest state.\n",
    "\n",
    "This enables:\n",
    "\n",
    "* Iterative reasoning\n",
    "* Self-correction\n",
    "* Reflection loops\n",
    "* Multi-agent negotiation\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Variants of Prompt Templates in LangGraph**\n",
    "\n",
    "| Variant                   | Purpose                     |\n",
    "| ------------------------- | --------------------------- |\n",
    "| **PromptTemplate**        | Single text prompt          |\n",
    "| **ChatPromptTemplate**    | Multi-message chat format   |\n",
    "| **FewShotPromptTemplate** | Example-based prompting     |\n",
    "| **Conditional Prompts**   | Branching instructions      |\n",
    "| **Composable Prompts**    | Modular prompt components   |\n",
    "| **Dynamic Prompts**       | Runtime-generated templates |\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Chat Prompt Example**\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a precise research assistant.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "    (\"assistant\", \"Think step by step.\")\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Prompt Templates in Cyclic & Agentic Graphs**\n",
    "\n",
    "Prompt templates are **re-evaluated on every loop iteration**.\n",
    "\n",
    "This allows:\n",
    "\n",
    "* Progressive refinement\n",
    "* Memory growth\n",
    "* Plan updates\n",
    "* Error correction\n",
    "\n",
    "Example loop:\n",
    "\n",
    "```\n",
    "Plan → Execute → Evaluate → Revise → Plan → ...\n",
    "```\n",
    "\n",
    "Each stage uses a **different template** bound to the same state.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Production Design Principles**\n",
    "\n",
    "| Principle       | Implementation               |\n",
    "| --------------- | ---------------------------- |\n",
    "| Determinism     | Fixed instruction blocks     |\n",
    "| Auditability    | Versioned templates          |\n",
    "| Safety          | Guardrails in system message |\n",
    "| Scalability     | Modular templates            |\n",
    "| Maintainability | Template registry            |\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Mental Model**\n",
    "\n",
    "> **Prompt Templates are the compiler frontend of LangGraph.**\n",
    "\n",
    "They translate **structured application state** into **LLM-executable instructions**, enabling LangGraph to behave as a **deterministic, stateful, multi-step reasoning engine** rather than a simple prompt pipeline.\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad3ec94-4bb2-4206-9798-29496124ef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent is an optimization algorithm used to minimize the loss function in machine learning models. It works by iteratively adjusting the model's parameters in the opposite direction of the gradient (or slope) of the loss function with respect to those parameters. The size of the adjustments is determined by a hyperparameter called the learning rate. The goal is to find the parameter values that result in the lowest possible loss, effectively improving the model's performance on training data.\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# LangGraph + PromptTemplate Demo (Single Cell)\n",
    "# ===========================\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Define shared state\n",
    "# ---------------------------\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    memory: str\n",
    "    answer: str\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Create a Prompt Template\n",
    "# ---------------------------\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"memory\"],\n",
    "    template=\"\"\"\n",
    "You are a precise technical tutor.\n",
    "\n",
    "Conversation memory:\n",
    "{memory}\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Provide a concise and accurate answer.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. LLM Node using template\n",
    "# ---------------------------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "def reasoning_node(state: State):\n",
    "    rendered_prompt = prompt.format(\n",
    "        question=state[\"question\"],\n",
    "        memory=state[\"memory\"]\n",
    "    )\n",
    "    response = llm.invoke(rendered_prompt)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Build LangGraph\n",
    "# ---------------------------\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"reason\", reasoning_node)\n",
    "builder.set_entry_point(\"reason\")\n",
    "builder.add_edge(\"reason\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Run the graph\n",
    "# ---------------------------\n",
    "result = graph.invoke({\n",
    "    \"question\": \"What is gradient descent?\",\n",
    "    \"memory\": \"User is learning machine learning basics.\"\n",
    "})\n",
    "\n",
    "print(result[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
