{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c3690f-b5d1-43e3-8913-d49a581b8ab7",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## **Streaming Tokens in LangGraph**\n",
    "\n",
    "**Streaming tokens** in LangGraph refers to the ability to **emit partial model outputs incrementally as they are generated**, rather than waiting for the full response.\n",
    "This enables **low-latency UX**, real-time agent monitoring, progressive rendering, and early intervention in long-running workflows.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Motivation and Intuition**\n",
    "\n",
    "Without streaming, the system behaves as:\n",
    "\n",
    "```\n",
    "Input → [LLM computes silently] → Final Output\n",
    "```\n",
    "\n",
    "With streaming:\n",
    "\n",
    "```\n",
    "Input → token₁ → token₂ → token₃ → ... → Final Output\n",
    "```\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "| Property      | Impact                            |\n",
    "| ------------- | --------------------------------- |\n",
    "| Latency       | Immediate feedback                |\n",
    "| UX            | Perceived responsiveness          |\n",
    "| Control       | Can interrupt or modify execution |\n",
    "| Observability | Inspect reasoning in real-time    |\n",
    "| Cost          | Early termination possible        |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Where Streaming Lives in LangGraph**\n",
    "\n",
    "Streaming operates at the **node execution layer**, primarily on:\n",
    "\n",
    "* **LLM nodes**\n",
    "* **Agent nodes**\n",
    "* **Tool-calling nodes**\n",
    "\n",
    "LangGraph **propagates streaming events** through the graph runtime.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Core Streaming Architecture**\n",
    "\n",
    "```\n",
    "Graph Runtime\n",
    "   ↓\n",
    "Node Executor\n",
    "   ↓\n",
    "LLM (stream=True)\n",
    "   ↓\n",
    "Token Events\n",
    "   ↓\n",
    "LangGraph Stream Channel\n",
    "   ↓\n",
    "User / UI / Logger\n",
    "```\n",
    "\n",
    "Each token becomes an **event** on the execution channel.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Minimal Working Example**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: list\n",
    "\n",
    "llm = ChatOpenAI(streaming=True)\n",
    "\n",
    "def chat_node(state):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"chat\", chat_node)\n",
    "builder.set_entry_point(\"chat\")\n",
    "builder.add_edge(\"chat\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "for event in graph.stream({\"messages\": []}):\n",
    "    print(event)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Consuming Token Streams**\n",
    "\n",
    "Each event is structured:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"event\": \"on_llm_new_token\",\n",
    "  \"token\": \"Hello\",\n",
    "  \"node\": \"chat\",\n",
    "  \"run_id\": \"...\"\n",
    "}\n",
    "```\n",
    "\n",
    "Typical consumer logic:\n",
    "\n",
    "```python\n",
    "for event in graph.stream(input):\n",
    "    if event[\"event\"] == \"on_llm_new_token\":\n",
    "        render(event[\"token\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Streaming with Tools and Agents**\n",
    "\n",
    "During tool-augmented generation:\n",
    "\n",
    "```\n",
    "Tokens → Tool call → Tool result → More tokens\n",
    "```\n",
    "\n",
    "LangGraph preserves this full sequence as a **continuous stream**, enabling:\n",
    "\n",
    "* Real-time agent debugging\n",
    "* Live tool visibility\n",
    "* Partial answer display\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Streaming in Cyclic Graphs**\n",
    "\n",
    "In loops:\n",
    "\n",
    "```\n",
    "Reason → Act → Observe → (loop)\n",
    "```\n",
    "\n",
    "Streaming remains **continuous across iterations**, allowing:\n",
    "\n",
    "* Live trace of reasoning evolution\n",
    "* Intervention before infinite loops\n",
    "* Progressive refinement\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Advanced Streaming Control**\n",
    "\n",
    "| Feature             | Description                      |\n",
    "| ------------------- | -------------------------------- |\n",
    "| Backpressure        | Slow consumers regulate producer |\n",
    "| Early termination   | Stop graph when condition met    |\n",
    "| Selective streaming | Stream only chosen nodes         |\n",
    "| Token filtering     | Remove sensitive tokens          |\n",
    "| Batching            | Aggregate tokens for performance |\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Production Use Cases**\n",
    "\n",
    "| Use Case          | Benefit                           |\n",
    "| ----------------- | --------------------------------- |\n",
    "| Chat UI           | Instant typing effect             |\n",
    "| Long analysis     | User confidence & engagement      |\n",
    "| Agent debugging   | Observe failures early            |\n",
    "| Human-in-the-loop | Mid-execution corrections         |\n",
    "| Monitoring        | Detect hallucination in real-time |\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Performance & Safety Considerations**\n",
    "\n",
    "* Streaming slightly increases CPU overhead\n",
    "* Must sanitize tokens before exposing\n",
    "* Enforce output length & recursion limits\n",
    "* Mask private data in live streams\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Conceptual Summary**\n",
    "\n",
    "LangGraph streaming transforms LLM execution from:\n",
    "\n",
    "> **Batch inference → Event-driven computation**\n",
    "\n",
    "This is essential for building **responsive, observable, and controllable AI systems**.\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f4085b7-84d8-494d-bb8b-70871ee4784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: \n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ====== LangGraph Token Streaming Demo (Robust & Version-Safe) ======\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.schema import HumanMessage\n",
    "\n",
    "# ---- State Definition ----\n",
    "class State(TypedDict):\n",
    "    messages: List\n",
    "\n",
    "# ---- Streaming LLM ----\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", streaming=True)\n",
    "\n",
    "# ---- Node Definition ----\n",
    "def chat_node(state: State):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "# ---- Build Graph ----\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"chat\", chat_node)\n",
    "builder.set_entry_point(\"chat\")\n",
    "builder.add_edge(\"chat\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# ---- Execute with Streaming ----\n",
    "print(\"Assistant:\", end=\" \", flush=True)\n",
    "\n",
    "for event in graph.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Explain LangGraph streaming in one sentence.\")]}\n",
    "):\n",
    "    # event may be dict, tuple, or object depending on LangGraph version\n",
    "    if isinstance(event, dict):\n",
    "        if event.get(\"type\") == \"on_llm_new_token\":\n",
    "            print(event[\"token\"], end=\"\", flush=True)\n",
    "\n",
    "    elif isinstance(event, tuple) and len(event) == 2:\n",
    "        event_type, payload = event\n",
    "        if event_type == \"on_llm_new_token\":\n",
    "            print(payload, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86a09f-8860-413f-b890-c0f1772ef495",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
