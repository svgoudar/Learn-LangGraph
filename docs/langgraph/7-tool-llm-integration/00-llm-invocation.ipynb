{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df3f9c9-46c9-4c28-b230-f263187676da",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## LLM Invocation\n",
    "\n",
    "**LLM invocation** in LangGraph is the controlled execution of a language model as a **node** inside a **stateful execution graph**, where inputs, outputs, failures, retries, and side-effects are managed through the **graph’s state machine**.\n",
    "\n",
    "This design enables **deterministic orchestration of non-deterministic models**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Conceptual Role**\n",
    "\n",
    "In LangGraph, an LLM is **not called directly**.\n",
    "It is invoked as part of a **node** whose behavior is governed by:\n",
    "\n",
    "| Layer    | Responsibility               |\n",
    "| -------- | ---------------------------- |\n",
    "| Graph    | Controls *when* the LLM runs |\n",
    "| State    | Controls *what data* it sees |\n",
    "| Node     | Defines *how* it is invoked  |\n",
    "| Edges    | Decide *what happens next*   |\n",
    "| Policies | Enforce safety & reliability |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Basic LLM Node Anatomy**\n",
    "\n",
    "```\n",
    "State → [ LLM Node ] → Updated State\n",
    "```\n",
    "\n",
    "An LLM node is simply a function that:\n",
    "\n",
    "1. Reads from the shared state\n",
    "2. Calls the model\n",
    "3. Returns partial state updates\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "```\n",
    "\n",
    "```python\n",
    "def llm_node(state):\n",
    "    response = llm.invoke(state[\"prompt\"])\n",
    "    return {\"output\": response.content}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Typed State Integration**\n",
    "\n",
    "```python\n",
    "class State(TypedDict):\n",
    "    prompt: str\n",
    "    output: str\n",
    "```\n",
    "\n",
    "LangGraph enforces **structured flow**:\n",
    "the LLM only receives data exposed by the state schema.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Graph Wiring**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"llm\", llm_node)\n",
    "builder.set_entry_point(\"llm\")\n",
    "builder.add_edge(\"llm\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "```\n",
    "\n",
    "```python\n",
    "graph.invoke({\"prompt\": \"Explain transformers in one sentence\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Prompt Engineering Inside the Node**\n",
    "\n",
    "```python\n",
    "def llm_node(state):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert teacher.\n",
    "    Question: {state['question']}\n",
    "    Answer concisely.\n",
    "    \"\"\"\n",
    "    result = llm.invoke(prompt)\n",
    "    return {\"answer\": result.content}\n",
    "```\n",
    "\n",
    "This keeps **prompt logic local**, while **control flow remains global**.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Advanced Invocation Patterns**\n",
    "\n",
    "| Pattern             | Purpose                     |\n",
    "| ------------------- | --------------------------- |\n",
    "| Multi-Model Routing | Cost / latency optimization |\n",
    "| Tool-Calling        | Structured external actions |\n",
    "| Streaming           | Real-time token flow        |\n",
    "| Batch Inference     | High throughput             |\n",
    "| Reflection          | Self-correction             |\n",
    "| ReAct               | Reason → Act loops          |\n",
    "| Fallback Models     | Reliability                 |\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Conditional LLM Routing**\n",
    "\n",
    "```python\n",
    "def router(state):\n",
    "    if state[\"difficulty\"] == \"hard\":\n",
    "        return \"gpt4\"\n",
    "    return \"gpt35\"\n",
    "```\n",
    "\n",
    "```python\n",
    "builder.add_conditional_edges(\"router\", router, {\n",
    "    \"gpt4\": \"llm_gpt4\",\n",
    "    \"gpt35\": \"llm_gpt35\"\n",
    "})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. LLM + Tools Invocation**\n",
    "\n",
    "```python\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculator(x: int, y: int):\n",
    "    return x + y\n",
    "```\n",
    "\n",
    "```python\n",
    "llm = ChatOpenAI().bind_tools([calculator])\n",
    "```\n",
    "\n",
    "The model decides **when** to call tools;\n",
    "LangGraph controls **how and where** those effects propagate.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Reliability Controls**\n",
    "\n",
    "| Control         | Mechanism                  |\n",
    "| --------------- | -------------------------- |\n",
    "| Retries         | Automatic retry nodes      |\n",
    "| Timeout         | Hard execution limits      |\n",
    "| Fallback        | Alternate model paths      |\n",
    "| Circuit Breaker | Prevent cascading failures |\n",
    "| Checkpointing   | Safe resume                |\n",
    "| Audit Log       | Full traceability          |\n",
    "\n",
    "```python\n",
    "graph.invoke(input, config={\"recursion_limit\": 25})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Why LangGraph Invocation Is Different**\n",
    "\n",
    "| Traditional LLM Call | LangGraph Invocation |\n",
    "| -------------------- | -------------------- |\n",
    "| Ad-hoc               | State-governed       |\n",
    "| Hidden side effects  | Explicit transitions |\n",
    "| No memory            | Persistent memory    |\n",
    "| No recovery          | Fault tolerant       |\n",
    "| Hard to debug        | Fully traceable      |\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Mental Model**\n",
    "\n",
    "> **LLM Invocation = A controlled, observable, recoverable computational step in a larger state machine**\n",
    "\n",
    "This makes LangGraph suitable for **autonomous agents, enterprise systems, and long-running reasoning processes**.\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c7d2c74-7989-4e57-8b08-833e58a17dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are a type of deep learning architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017, which primarily revolutionized the fields of natural language processing (NLP) and beyond. Unlike previous sequential models like RNNs and LSTMs, transformers leverage self-attention mechanisms to process data in parallel, enabling them to capture long-range dependencies in sequences more effectively. The architecture consists of an encoder-decoder structure: the encoder processes the input data to create a rich set of representations, while the decoder generates the output sequence based on the encoder's output. Transformers utilize multi-head attention and position-wise feedforward networks, allowing the model to focus on different parts of the input when generating outputs and to learn complex relationships within the data. Additionally, transformers use positional encodings to account for the order of input sequences, as the architecture itself does not contain any recurrent or convolutional components. Their scalability and efficiency have led to the foundation of models like BERT, GPT, and many others, which have achieved state-of-the-art performance in various tasks across different domains.\n"
     ]
    }
   ],
   "source": [
    "# ===== Complete LangGraph LLM Invocation Demo =====\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Define State Schema\n",
    "# ------------------------------\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    difficulty: str\n",
    "    answer: str\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Initialize Models\n",
    "# ------------------------------\n",
    "fast_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "strong_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Define LLM Nodes\n",
    "# ------------------------------\n",
    "def fast_llm(state: State):\n",
    "    response = fast_model.invoke(f\"Answer simply: {state['question']}\")\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "def strong_llm(state: State):\n",
    "    response = strong_model.invoke(f\"Provide a detailed answer: {state['question']}\")\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Routing Logic\n",
    "# ------------------------------\n",
    "def route(state: State):\n",
    "    return \"strong\" if state[\"difficulty\"] == \"hard\" else \"fast\"\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Build Graph\n",
    "# ------------------------------\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"fast\", fast_llm)\n",
    "builder.add_node(\"strong\", strong_llm)\n",
    "\n",
    "builder.set_entry_point(\"fast\")\n",
    "\n",
    "builder.add_conditional_edges(\"fast\", route, {\n",
    "    \"fast\": \"fast\",\n",
    "    \"strong\": \"strong\"\n",
    "})\n",
    "\n",
    "builder.add_edge(\"strong\", END)\n",
    "builder.add_edge(\"fast\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Execute\n",
    "# ------------------------------\n",
    "result = graph.invoke({\n",
    "    \"question\": \"Explain transformers in one paragraph.\",\n",
    "    \"difficulty\": \"hard\"\n",
    "})\n",
    "\n",
    "print(result[\"answer\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
