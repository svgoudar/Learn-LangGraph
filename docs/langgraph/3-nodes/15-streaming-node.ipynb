{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbcfba29",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## **Streaming Node in LangGraph**\n",
    "\n",
    "A **Streaming Node** in LangGraph is a specialized execution node that emits **partial outputs incrementally** as the underlying LLM or tool produces them, rather than waiting for the entire computation to finish.\n",
    "This enables **real-time feedback, low-latency UIs, conversational responsiveness, and long-running task monitoring**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Motivation: Why Streaming Exists**\n",
    "\n",
    "Traditional LLM nodes behave as **batch functions**:\n",
    "\n",
    "```\n",
    "input → compute → full output → return\n",
    "```\n",
    "\n",
    "Streaming nodes change the model to:\n",
    "\n",
    "```\n",
    "input → compute → token₁ → token₂ → token₃ → … → final output\n",
    "```\n",
    "\n",
    "**Benefits**\n",
    "\n",
    "| Problem                | Solved by Streaming         |\n",
    "| ---------------------- | --------------------------- |\n",
    "| High perceived latency | Immediate partial responses |\n",
    "| Long responses         | Progressive rendering       |\n",
    "| User trust             | Visible progress            |\n",
    "| Monitoring             | Real-time execution insight |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Conceptual Model**\n",
    "\n",
    "```\n",
    "State_in\n",
    "   ↓\n",
    "Streaming Node\n",
    "   ↓\n",
    "[ token → token → token → ... ]\n",
    "   ↓\n",
    "State_out (final aggregated result)\n",
    "```\n",
    "\n",
    "Streaming does **not** change the graph structure; it changes **how a node produces output**.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Where Streaming Fits in LangGraph**\n",
    "\n",
    "Streaming nodes are typically:\n",
    "\n",
    "* LLM calls\n",
    "* Tool executions with long outputs\n",
    "* Multi-step generators\n",
    "* Agent reasoning traces\n",
    "\n",
    "They integrate seamlessly with:\n",
    "\n",
    "* Cyclic graphs\n",
    "* Agent loops\n",
    "* Human-in-the-loop systems\n",
    "* UI applications\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Core Streaming API in LangGraph**\n",
    "\n",
    "LangGraph exposes streaming through **graph execution**, not special node types.\n",
    "\n",
    "```python\n",
    "for event in graph.stream(input_state):\n",
    "    print(event)\n",
    "```\n",
    "\n",
    "Each `event` represents an **incremental state update**.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Example: Streaming an LLM Node**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph\n",
    "from typing import TypedDict\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    answer: str\n",
    "\n",
    "llm = ChatOpenAI(streaming=True)\n",
    "\n",
    "def llm_node(state):\n",
    "    response = llm.invoke(state[\"query\"])\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"llm\", llm_node)\n",
    "builder.set_entry_point(\"llm\")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "for event in graph.stream({\"query\": \"Explain transformers\"}):\n",
    "    print(event)\n",
    "```\n",
    "\n",
    "**What Happens Internally**\n",
    "\n",
    "| Step                      | Description           |\n",
    "| ------------------------- | --------------------- |\n",
    "| Model produces token      | LLM streams token     |\n",
    "| LangGraph captures update | Partial state emitted |\n",
    "| Downstream consumers      | UI / logs update      |\n",
    "| Final token               | State converges       |\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Streaming with Agents (ReAct Loop)**\n",
    "\n",
    "```python\n",
    "for event in graph.stream(input):\n",
    "    if \"messages\" in event:\n",
    "        print(event[\"messages\"][-1].content, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "Used for:\n",
    "\n",
    "* Live chat interfaces\n",
    "* Debugging agent reasoning\n",
    "* Tool execution progress\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Event Types in Streaming**\n",
    "\n",
    "| Event        | Meaning              |\n",
    "| ------------ | -------------------- |\n",
    "| State update | Partial state change |\n",
    "| Token event  | LLM token            |\n",
    "| Node start   | Execution begins     |\n",
    "| Node end     | Execution completes  |\n",
    "| Error event  | Failure              |\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Streaming in Cyclic Graphs**\n",
    "\n",
    "Streaming becomes crucial in loops:\n",
    "\n",
    "```\n",
    "Reason → Act → Observe → Reason → …\n",
    "```\n",
    "\n",
    "Each cycle streams intermediate reasoning and tool output, enabling:\n",
    "\n",
    "* Live visualization of agent behavior\n",
    "* Early termination by human\n",
    "* Adaptive control\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Production Use Cases**\n",
    "\n",
    "| Use Case            | Why Streaming                 |\n",
    "| ------------------- | ----------------------------- |\n",
    "| Chatbots            | Real-time conversation        |\n",
    "| Research assistants | Progressive report generation |\n",
    "| Code assistants     | Show code while writing       |\n",
    "| Monitoring agents   | Execution transparency        |\n",
    "| Autonomous systems  | Live control & safety         |\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Safety & Performance Controls**\n",
    "\n",
    "| Control         | Purpose                |\n",
    "| --------------- | ---------------------- |\n",
    "| Token limits    | Prevent runaway output |\n",
    "| Backpressure    | Avoid UI overload      |\n",
    "| Throttling      | Rate control           |\n",
    "| Human interrupt | Stop execution         |\n",
    "| Checkpointing   | Resume after failure   |\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Mental Model**\n",
    "\n",
    "A Streaming Node converts LangGraph from:\n",
    "\n",
    "> **\"Batch workflow engine\"**\n",
    "> to\n",
    "> **\"Interactive real-time reasoning system\"**\n",
    "\n",
    "---\n",
    "\n",
    "### **12. Comparison: Streaming vs Non-Streaming**\n",
    "\n",
    "| Aspect    | Non-Streaming  | Streaming       |\n",
    "| --------- | -------------- | --------------- |\n",
    "| Latency   | High           | Low             |\n",
    "| UX        | Blocking       | Interactive     |\n",
    "| Debugging | Hard           | Transparent     |\n",
    "| Autonomy  | Limited        | Strong          |\n",
    "| Safety    | Low visibility | High visibility |\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a011ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    answer: str\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(streaming=True)\n",
    "\n",
    "def streaming_llm_node(state: State):\n",
    "    response = llm.invoke(state[\"query\"])\n",
    "    return {\"answer\": response.content}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d79047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"llm\", streaming_llm_node)\n",
    "builder.set_entry_point(\"llm\")\n",
    "builder.add_edge(\"llm\", END)\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01218ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mechanism is a technique used in neural networks to improve the performance of machine learning models, particularly in tasks that involve sequential data or have long-range dependencies. \n",
      "\n",
      "At its core, attention mechanism allows the model to focus on specific parts of the input data that are more relevant for the task at hand, rather than considering the entire input sequence at once. This is inspired by the way human brains process information, where we selectively focus our attention on certain elements while ignoring others.\n",
      "\n",
      "In practice, attention mechanism works by assigning a weight to each input element based on its importance for the current step of the computation. These weights are then used to calculate a context vector, which is a weighted sum of all input elements. This context vector is then used in the computation of the model's output, allowing it to effectively capture dependencies and relationships between elements in the input data.\n",
      "\n",
      "Overall, attention mechanism helps improve model performance by allowing it to selectively focus on relevant information, which can lead to better accuracy and performance in complex tasks such as machine translation, image captioning, and speech recognition."
     ]
    }
   ],
   "source": [
    "for event in graph.stream({\"query\": \"Explain attention mechanism\"}):\n",
    "    if \"answer\" in event[\"llm\"].keys():\n",
    "        print(event['llm'][\"answer\"], end=\"\", flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
