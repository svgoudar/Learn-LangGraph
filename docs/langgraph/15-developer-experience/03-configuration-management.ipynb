{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ac2711-892f-48e4-ac91-36edd83e9d7a",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Configuration Management\n",
    "\n",
    "Configuration management in **LangGraph** controls **how a graph executes at runtime without changing its structure**.\n",
    "It separates **workflow logic** from **execution behavior**, enabling safe experimentation, scaling, observability, and production governance.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Why Configuration Management Matters**\n",
    "\n",
    "Without configuration, every change requires **rewriting the graph**.\n",
    "With configuration, you can:\n",
    "\n",
    "| Goal                    | Achieved By    |\n",
    "| ----------------------- | -------------- |\n",
    "| Switch models           | Runtime config |\n",
    "| Change recursion limits | Runtime config |\n",
    "| Control retries         | Runtime config |\n",
    "| Inject callbacks        | Runtime config |\n",
    "| Enable tracing          | Runtime config |\n",
    "| Tune performance        | Runtime config |\n",
    "\n",
    "> **Design principle:**\n",
    "> *Graph = algorithm, Config = operating policy*\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Core Components of LangGraph Configuration**\n",
    "\n",
    "LangGraph uses a structured **runtime configuration dictionary**:\n",
    "\n",
    "```python\n",
    "config = {\n",
    "    \"configurable\": {...},\n",
    "    \"recursion_limit\": 25,\n",
    "    \"callbacks\": [...],\n",
    "    \"tags\": [...],\n",
    "    \"metadata\": {...}\n",
    "}\n",
    "```\n",
    "\n",
    "| Section         | Purpose                     |\n",
    "| --------------- | --------------------------- |\n",
    "| configurable    | Custom runtime parameters   |\n",
    "| recursion_limit | Maximum loop steps          |\n",
    "| callbacks       | Tracing, logging, streaming |\n",
    "| tags            | Label executions            |\n",
    "| metadata        | Attach structured info      |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Configurable Fields**\n",
    "\n",
    "Nodes can declare **configurable parameters**:\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "model = ChatOpenAI().configurable_fields(\n",
    "    temperature=ConfigurableField(id=\"temperature\"),\n",
    "    model_name=ConfigurableField(id=\"model\")\n",
    ")\n",
    "```\n",
    "\n",
    "Runtime injection:\n",
    "\n",
    "```python\n",
    "graph.invoke(input, config={\n",
    "    \"configurable\": {\n",
    "        \"temperature\": 0.2,\n",
    "        \"model\": \"gpt-4o\"\n",
    "    }\n",
    "})\n",
    "```\n",
    "\n",
    "**Effect:** same graph, different behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Controlling Cycles & Safety**\n",
    "\n",
    "```python\n",
    "graph.invoke(input, config={\n",
    "    \"recursion_limit\": 20\n",
    "})\n",
    "```\n",
    "\n",
    "| Setting         | Role                    |\n",
    "| --------------- | ----------------------- |\n",
    "| recursion_limit | Prevent infinite loops  |\n",
    "| timeout         | Prevent hung executions |\n",
    "| max_concurrency | Control parallelism     |\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Observability & Debugging**\n",
    "\n",
    "```python\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "graph.invoke(input, config={\n",
    "    \"callbacks\": [StdOutCallbackHandler()],\n",
    "    \"tags\": [\"experiment-A\"],\n",
    "    \"metadata\": {\"user_id\": \"42\"}\n",
    "})\n",
    "```\n",
    "\n",
    "This enables:\n",
    "\n",
    "* Execution tracing\n",
    "* Token usage tracking\n",
    "* Error localization\n",
    "* Performance profiling\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Environment-Based Configuration**\n",
    "\n",
    "Typical production pattern:\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"model\": os.getenv(\"LLM_MODEL\"),\n",
    "        \"temperature\": float(os.getenv(\"TEMP\"))\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "| Environment | Behavior                  |\n",
    "| ----------- | ------------------------- |\n",
    "| Dev         | High logging, low limits  |\n",
    "| Staging     | Medium limits             |\n",
    "| Prod        | Strict limits, monitoring |\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Configuration in Multi-Agent Systems**\n",
    "\n",
    "Each agent can receive **independent configuration**:\n",
    "\n",
    "```python\n",
    "graph.invoke(input, config={\n",
    "    \"configurable\": {\n",
    "        \"planner_model\": \"gpt-4\",\n",
    "        \"executor_model\": \"gpt-3.5-turbo\"\n",
    "    }\n",
    "})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Production Configuration Checklist**\n",
    "\n",
    "| Layer         | Configuration                  |\n",
    "| ------------- | ------------------------------ |\n",
    "| LLM           | model, temperature, max_tokens |\n",
    "| Safety        | recursion_limit, timeout       |\n",
    "| Scale         | max_concurrency                |\n",
    "| Observability | callbacks, tags                |\n",
    "| Cost          | token limits                   |\n",
    "| Governance    | metadata, audit tags           |\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Conceptual Summary**\n",
    "\n",
    "LangGraph configuration transforms workflows from **hard-coded pipelines** into **flexible, policy-driven systems**:\n",
    "\n",
    "> **Graph defines what happens.\n",
    "> Configuration defines how it happens.**\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e79df7-4424-48cb-a97f-54c8242661c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LangGraph chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new reason chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Final Output:\n",
      " Overfitting occurs when a machine learning model learns the training data too well, capturing noise and outliers, which results in poor generalization to new, unseen data.\n"
     ]
    }
   ],
   "source": [
    "# One-cell demonstration: LangGraph configuration management in action\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_classic.callbacks import StdOutCallbackHandler\n",
    "\n",
    "# -------------------------\n",
    "# 1. Define State\n",
    "# -------------------------\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "# -------------------------\n",
    "# 2. Create configurable LLM\n",
    "# -------------------------\n",
    "llm = ChatOpenAI().configurable_fields(\n",
    "    model_name=ConfigurableField(id=\"model\"),\n",
    "    temperature=ConfigurableField(id=\"temperature\"),\n",
    "    max_tokens=ConfigurableField(id=\"max_tokens\")\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 3. Define graph node\n",
    "# -------------------------\n",
    "def reasoning_node(state: State):\n",
    "    response = llm.invoke(state[\"question\"])\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# -------------------------\n",
    "# 4. Build graph\n",
    "# -------------------------\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"reason\", reasoning_node)\n",
    "builder.set_entry_point(\"reason\")\n",
    "builder.add_edge(\"reason\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# -------------------------\n",
    "# 5. Run with different configurations\n",
    "# -------------------------\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 100\n",
    "    },\n",
    "    \"recursion_limit\": 5,\n",
    "    \"callbacks\": [StdOutCallbackHandler()],\n",
    "    \"tags\": [\"demo\", \"config-management\"],\n",
    "    \"metadata\": {\"env\": \"development\"}\n",
    "}\n",
    "\n",
    "result = graph.invoke(\n",
    "    {\"question\": \"Explain overfitting in one sentence.\"},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Output:\\n\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cdf847-9a0f-4570-9239-4ea94c58afe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
