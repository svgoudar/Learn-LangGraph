{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ace44f-c0e2-47ab-9b7e-300213cad323",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## **Cache in LangGraph**\n",
    "\n",
    "In LangGraph, **caching** is a production-critical optimization technique that **stores and reuses intermediate computation results**—especially LLM responses, tool outputs, and node executions—so that repeated or equivalent computations are not recomputed.\n",
    "Caching improves **latency, cost efficiency, reliability, and scalability** of graph-based LLM systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Why Caching Is Essential in LangGraph**\n",
    "\n",
    "LangGraph workflows are:\n",
    "\n",
    "* **Stateful**\n",
    "* **Iterative (often cyclic)**\n",
    "* **Tool-heavy**\n",
    "* **LLM-expensive**\n",
    "\n",
    "Without caching:\n",
    "\n",
    "| Problem              | Impact                |\n",
    "| -------------------- | --------------------- |\n",
    "| Repeated LLM calls   | High cost             |\n",
    "| Loops & retries      | Exponential latency   |\n",
    "| Multi-agent systems  | Unstable throughput   |\n",
    "| Production workloads | Unpredictable scaling |\n",
    "\n",
    "Caching converts LangGraph from a **research prototype** into a **production system**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. What Can Be Cached**\n",
    "\n",
    "| Layer           | What is Cached              |\n",
    "| --------------- | --------------------------- |\n",
    "| Node Output     | LLM responses, tool results |\n",
    "| Graph Step      | Entire node execution       |\n",
    "| Subgraph        | Multi-step execution        |\n",
    "| State Snapshots | Checkpoints                 |\n",
    "| LLM Calls       | Prompt → response           |\n",
    "| Tool Calls      | Input → output              |\n",
    "| Retrieval       | Query → documents           |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Where Cache Lives**\n",
    "\n",
    "| Cache Location  | Use Case                  |\n",
    "| --------------- | ------------------------- |\n",
    "| In-Memory       | Fast, per-process         |\n",
    "| Redis           | Distributed, multi-worker |\n",
    "| Database        | Durable                   |\n",
    "| Vector DB       | Semantic cache            |\n",
    "| LangChain Cache | LLM-level caching         |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Cache in LangGraph Architecture**\n",
    "\n",
    "```\n",
    "Client\n",
    "  |\n",
    "LangGraph Runtime\n",
    "  |\n",
    "[Cache Layer] ──► LLM / Tools\n",
    "  |\n",
    "State Store\n",
    "```\n",
    "\n",
    "When a node executes:\n",
    "\n",
    "1. Generate **cache key**\n",
    "2. Check cache\n",
    "3. If hit → return stored result\n",
    "4. If miss → compute → store → return\n",
    "\n",
    "---\n",
    "\n",
    "### **5. LLM Caching in LangGraph (LangChain Cache)**\n",
    "\n",
    "```python\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "set_llm_cache(SQLiteCache(\"llm_cache.db\"))\n",
    "```\n",
    "\n",
    "Now every LLM call inside LangGraph is cached.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Node-Level Caching Pattern**\n",
    "\n",
    "```python\n",
    "cache = {}\n",
    "\n",
    "def expensive_node(state):\n",
    "    key = state[\"query\"]\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "    result = heavy_computation(state)\n",
    "    cache[key] = result\n",
    "    return result\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Production Cache with Redis**\n",
    "\n",
    "```python\n",
    "import redis, json\n",
    "\n",
    "r = redis.Redis()\n",
    "\n",
    "def cached_node(state):\n",
    "    key = f\"node:{state['input']}\"\n",
    "    if r.exists(key):\n",
    "        return json.loads(r.get(key))\n",
    "    result = compute(state)\n",
    "    r.set(key, json.dumps(result))\n",
    "    return result\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Cache with Cyclic Graphs**\n",
    "\n",
    "In cyclic graphs, caching prevents infinite recomputation.\n",
    "\n",
    "```\n",
    "Reason → Act → Observe\n",
    "   ↑                ↓\n",
    "   └──── Cache ─────┘\n",
    "```\n",
    "\n",
    "Same state + same input = cached response → fast exit.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Semantic Cache (Advanced)**\n",
    "\n",
    "Use vector similarity instead of exact match.\n",
    "\n",
    "```\n",
    "New Query\n",
    "   ↓\n",
    "Vector Search\n",
    "   ↓\n",
    "Similar Prompt Found?\n",
    "   ↓\n",
    "Return Cached Answer\n",
    "```\n",
    "\n",
    "Great for chatbots & assistants.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Cache Invalidation Strategies**\n",
    "\n",
    "| Strategy          | When              |\n",
    "| ----------------- | ----------------- |\n",
    "| TTL               | Time-based expiry |\n",
    "| Versioning        | Graph update      |\n",
    "| State Hash Change | Input changed     |\n",
    "| Manual Clear      | Debugging         |\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Performance Impact**\n",
    "\n",
    "| Metric     | Without Cache | With Cache |\n",
    "| ---------- | ------------- | ---------- |\n",
    "| Latency    | High          | Low        |\n",
    "| Cost       | High          | Low        |\n",
    "| Throughput | Low           | High       |\n",
    "| Stability  | Fragile       | Robust     |\n",
    "\n",
    "---\n",
    "\n",
    "### **12. Mental Model**\n",
    "\n",
    "Caching transforms LangGraph from:\n",
    "\n",
    "> **Compute everything every time**\n",
    "\n",
    "into:\n",
    "\n",
    "> **Compute once, reuse everywhere**\n",
    "\n",
    "This is fundamental for **scaling autonomous, cyclic, multi-agent workflows**.\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d9b70-8a9f-42d6-8c62-4e95e7961e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# LangGraph Cache Demonstration\n",
    "# ==============================\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_classic.globals import set_llm_cache\n",
    "from langchain_classic.cache import SQLiteCache\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# -------- LLM Cache Setup --------\n",
    "set_llm_cache(SQLiteCache(\"llm_cache.db\"))\n",
    "\n",
    "# -------- State Schema --------\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    response: str\n",
    "\n",
    "# -------- Model --------\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# -------- Node-Level Cache --------\n",
    "node_cache = {}\n",
    "\n",
    "def cached_llm_node(state: State):\n",
    "    key = state[\"query\"]\n",
    "    \n",
    "    # Node cache check\n",
    "    if key in node_cache:\n",
    "        print(\"Node cache hit\")\n",
    "        return {\"response\": node_cache[key]}\n",
    "    \n",
    "    print(\"LLM called\")\n",
    "    result = llm.invoke(state[\"query\"]).content\n",
    "    \n",
    "    node_cache[key] = result\n",
    "    return {\"response\": result}\n",
    "\n",
    "# -------- Build Graph --------\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"llm_node\", cached_llm_node)\n",
    "builder.set_entry_point(\"llm_node\")\n",
    "builder.add_edge(\"llm_node\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# -------- Run Twice --------\n",
    "print(\"\\nFirst run:\")\n",
    "out1 = graph.invoke({\"query\": \"Explain transformers in one sentence.\"})\n",
    "\n",
    "print(\"\\nSecond run:\")\n",
    "out2 = graph.invoke({\"query\": \"Explain transformers in one sentence.\"})\n",
    "\n",
    "print(\"\\nFinal Output:\")\n",
    "print(out2[\"response\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
