{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3d8f47-da2c-4d79-996e-01e6d3429c6b",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Metrics\n",
    "\n",
    "In LangGraph, **metrics** are quantitative signals that measure the **health, performance, reliability, and cost-efficiency** of graph executions.\n",
    "They transform LLM workflows from experimental pipelines into **production-grade systems**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Why Metrics Are Essential**\n",
    "\n",
    "LangGraph workflows are **long-running, stateful, multi-agent, and stochastic**.\n",
    "Without metrics, you cannot answer:\n",
    "\n",
    "| Question                     | Why It Matters       |\n",
    "| ---------------------------- | -------------------- |\n",
    "| Is the system reliable?      | Production stability |\n",
    "| Where is time spent?         | Latency optimization |\n",
    "| Which agents fail?           | Fault diagnosis      |\n",
    "| How much does each run cost? | Budget control       |\n",
    "| Is performance degrading?    | Regression detection |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Metric Categories**\n",
    "\n",
    "| Category            | Purpose                 |\n",
    "| ------------------- | ----------------------- |\n",
    "| Execution Metrics   | Runtime behavior        |\n",
    "| State Metrics       | Data evolution          |\n",
    "| LLM Metrics         | Model efficiency        |\n",
    "| Agent Metrics       | Multi-agent performance |\n",
    "| Reliability Metrics | Failure behavior        |\n",
    "| Cost Metrics        | Financial control       |\n",
    "| User Metrics        | Experience quality      |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Execution Metrics**\n",
    "\n",
    "| Metric        | Description                  |\n",
    "| ------------- | ---------------------------- |\n",
    "| Graph Latency | Total runtime per invocation |\n",
    "| Node Latency  | Time per node                |\n",
    "| Queue Time    | Scheduling delay             |\n",
    "| Concurrency   | Parallel tasks               |\n",
    "| Throughput    | Runs per second              |\n",
    "| Step Count    | Total transitions            |\n",
    "| Loop Count    | Cycles executed              |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. State & Control Metrics**\n",
    "\n",
    "| Metric               | Meaning              |\n",
    "| -------------------- | -------------------- |\n",
    "| State Size           | Memory footprint     |\n",
    "| State Versions       | State evolution      |\n",
    "| Checkpoint Frequency | Recovery granularity |\n",
    "| Rollback Count       | Failure recovery     |\n",
    "| Interrupt Count      | Human interventions  |\n",
    "\n",
    "---\n",
    "\n",
    "### **5. LLM & Tool Metrics**\n",
    "\n",
    "| Metric             | Description           |\n",
    "| ------------------ | --------------------- |\n",
    "| Prompt Tokens      | Input tokens          |\n",
    "| Completion Tokens  | Output tokens         |\n",
    "| Total Tokens       | Cost driver           |\n",
    "| LLM Latency        | Model response time   |\n",
    "| Tool Calls         | External dependencies |\n",
    "| Tool Failure Rate  | Tool reliability      |\n",
    "| Hallucination Rate | Quality indicator     |\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Agent Metrics**\n",
    "\n",
    "| Metric             | Description             |\n",
    "| ------------------ | ----------------------- |\n",
    "| Agent Utilization  | Load per agent          |\n",
    "| Agent Failure Rate | Robustness              |\n",
    "| Delegation Depth   | Coordination complexity |\n",
    "| Conflict Rate      | Consensus difficulty    |\n",
    "| Consensus Time     | Decision latency        |\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Reliability Metrics**\n",
    "\n",
    "| Metric                | Meaning            |\n",
    "| --------------------- | ------------------ |\n",
    "| Success Rate          | Completed runs     |\n",
    "| Error Rate            | Failures           |\n",
    "| Retry Count           | Fault tolerance    |\n",
    "| Timeout Count         | Performance issues |\n",
    "| Circuit Breaker Trips | System stress      |\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Cost & Efficiency Metrics**\n",
    "\n",
    "| Metric              | Meaning              |\n",
    "| ------------------- | -------------------- |\n",
    "| Cost per Run        | Economic efficiency  |\n",
    "| Cost per Node       | Expensive components |\n",
    "| Cost per Agent      | Optimization target  |\n",
    "| Cache Hit Rate      | Cost reduction       |\n",
    "| Compute Utilization | Resource efficiency  |\n",
    "\n",
    "---\n",
    "\n",
    "### **9. User & Quality Metrics**\n",
    "\n",
    "| Metric              | Purpose         |\n",
    "| ------------------- | --------------- |\n",
    "| User Satisfaction   | Outcome quality |\n",
    "| Completion Rate     | Task success    |\n",
    "| Correction Rate     | Model accuracy  |\n",
    "| Human Approval Rate | Compliance      |\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Instrumentation Workflow**\n",
    "\n",
    "```python\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "metrics = StdOutCallbackHandler()\n",
    "\n",
    "graph.invoke(input, config={\"callbacks\": [metrics]})\n",
    "```\n",
    "\n",
    "For production, integrate with:\n",
    "\n",
    "* **Prometheus** → metrics storage\n",
    "* **Grafana** → dashboards\n",
    "* **OpenTelemetry** → distributed tracing\n",
    "* **LangSmith** → LLM-specific telemetry\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Example Metric Extraction**\n",
    "\n",
    "```python\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "\n",
    "tracer = LangChainTracer()\n",
    "graph.invoke(data, config={\"callbacks\": [tracer]})\n",
    "\n",
    "run_id = tracer.latest_run.id\n",
    "```\n",
    "\n",
    "Metrics collected:\n",
    "\n",
    "* Node timings\n",
    "* Token usage\n",
    "* Tool calls\n",
    "* Error events\n",
    "* State transitions\n",
    "\n",
    "---\n",
    "\n",
    "### **12. Metric-Driven Optimization Loop**\n",
    "\n",
    "```\n",
    "Measure → Analyze → Optimize → Validate → Deploy → Repeat\n",
    "```\n",
    "\n",
    "This transforms LangGraph from **workflow engine** into **autonomous production platform**.\n",
    "\n",
    "---\n",
    "\n",
    "### **13. Summary**\n",
    "\n",
    "LangGraph metrics provide **observability for intelligence**:\n",
    "\n",
    "> You cannot improve what you cannot measure.\n",
    "\n",
    "Metrics enable:\n",
    "\n",
    "* Reliability engineering\n",
    "* Cost governance\n",
    "* Performance tuning\n",
    "* Safety monitoring\n",
    "* Enterprise-grade operations\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39975669-0008-4417-89c6-9d1e9f4f54f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in MetricsCollector.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in MetricsCollector.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in MetricsCollector.on_chain_end callback: IndexError('list index out of range')\n",
      "Error in MetricsCollector.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in MetricsCollector.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in MetricsCollector.on_chain_end callback: IndexError('list index out of range')\n",
      "Error in MetricsCollector.on_chain_end callback: IndexError('list index out of range')\n",
      "Error in MetricsCollector.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in MetricsCollector.on_chain_end callback: IndexError('list index out of range')\n",
      "Error in MetricsCollector.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in MetricsCollector.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in MetricsCollector.on_chain_end callback: IndexError('list index out of range')\n",
      "Error in MetricsCollector.on_chain_end callback: IndexError('list index out of range')\n",
      "Error in MetricsCollector.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in MetricsCollector.on_chain_end callback: IndexError('list index out of range')\n",
      "Error in MetricsCollector.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in MetricsCollector.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in MetricsCollector.on_chain_end callback: IndexError('list index out of range')\n",
      "Error in MetricsCollector.on_chain_end callback: IndexError('list index out of range')\n",
      "Error in MetricsCollector.on_chain_end callback: IndexError('list index out of range')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== METRICS SUMMARY ===\n",
      "Total execution time: 0.928s\n",
      "Prompt tokens: 0\n",
      "Completion tokens: 0\n",
      "Total tokens: 0\n",
      "\n",
      "Final State: {'x': 3}\n"
     ]
    }
   ],
   "source": [
    "# One-cell LangGraph metrics demonstration\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_classic.callbacks.base import BaseCallbackHandler\n",
    "import time\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Custom Metrics Collector\n",
    "# ----------------------------\n",
    "\n",
    "class MetricsCollector(BaseCallbackHandler):\n",
    "    def __init__(self):\n",
    "        self.node_times = {}\n",
    "        self.total_start = time.time()\n",
    "        self.token_usage = {\"prompt\": 0, \"completion\": 0}\n",
    "\n",
    "    def on_chain_start(self, serialized, inputs, **kwargs):\n",
    "        self.node_times[serialized.get(\"name\", \"unknown\")] = time.time()\n",
    "\n",
    "    def on_chain_end(self, outputs, **kwargs):\n",
    "        name = list(self.node_times.keys())[-1]\n",
    "        elapsed = time.time() - self.node_times[name]\n",
    "        print(f\"[METRIC] Node '{name}' latency: {elapsed:.3f}s\")\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        usage = response.llm_output[\"token_usage\"]\n",
    "        self.token_usage[\"prompt\"] += usage[\"prompt_tokens\"]\n",
    "        self.token_usage[\"completion\"] += usage[\"completion_tokens\"]\n",
    "\n",
    "    def summary(self):\n",
    "        total = time.time() - self.total_start\n",
    "        print(\"\\n=== METRICS SUMMARY ===\")\n",
    "        print(f\"Total execution time: {total:.3f}s\")\n",
    "        print(f\"Prompt tokens: {self.token_usage['prompt']}\")\n",
    "        print(f\"Completion tokens: {self.token_usage['completion']}\")\n",
    "        print(f\"Total tokens: {self.token_usage['prompt'] + self.token_usage['completion']}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Graph Definition\n",
    "# ----------------------------\n",
    "\n",
    "class State(TypedDict):\n",
    "    x: int\n",
    "\n",
    "def step(state):\n",
    "    time.sleep(0.3)\n",
    "    return {\"x\": state[\"x\"] + 1}\n",
    "\n",
    "def check(state):\n",
    "    return {\"done\": state[\"x\"] >= 3}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"step\", step)\n",
    "builder.add_node(\"check\", check)\n",
    "\n",
    "builder.set_entry_point(\"step\")\n",
    "builder.add_edge(\"step\", \"check\")\n",
    "builder.add_conditional_edges(\"check\", lambda s: END if s[\"done\"] else \"step\", {\n",
    "    \"step\": \"step\",\n",
    "    END: END\n",
    "})\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Run With Metrics\n",
    "# ----------------------------\n",
    "\n",
    "metrics = MetricsCollector()\n",
    "\n",
    "result = graph.invoke({\"x\": 0}, config={\"callbacks\": [metrics]})\n",
    "metrics.summary()\n",
    "\n",
    "print(\"\\nFinal State:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f580e-4aea-451a-9fa6-397dba54ee95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
