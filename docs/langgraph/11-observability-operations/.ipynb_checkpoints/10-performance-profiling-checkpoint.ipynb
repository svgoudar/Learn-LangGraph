{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4bf16c8-249b-41ae-8200-e5b5acb55f8e",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Performance Profiling\n",
    "\n",
    "**Performance profiling** in LangGraph is the systematic measurement, analysis, and optimization of **execution time, resource usage, cost, and throughput** of LLM-based workflows.\n",
    "It ensures that complex agentic systems remain **fast, reliable, scalable, and cost-efficient** in production.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Why Performance Profiling Is Critical**\n",
    "\n",
    "LangGraph systems are **stateful, multi-step, tool-heavy, and often cyclic**.\n",
    "Without profiling, they suffer from:\n",
    "\n",
    "* Unbounded latency\n",
    "* Excessive token cost\n",
    "* Hidden bottlenecks in tools or agents\n",
    "* Cascading failures under load\n",
    "\n",
    "Profiling makes performance **observable and controllable**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Performance Dimensions Measured**\n",
    "\n",
    "| Dimension    | What It Measures           |\n",
    "| ------------ | -------------------------- |\n",
    "| Latency      | End-to-end execution time  |\n",
    "| Node Time    | Time per node              |\n",
    "| LLM Time     | Model inference delay      |\n",
    "| Tool Time    | External call delay        |\n",
    "| Token Usage  | Prompt + completion tokens |\n",
    "| Cost         | Monetary cost per run      |\n",
    "| Memory       | State + cache size         |\n",
    "| Throughput   | Requests per second        |\n",
    "| Concurrency  | Parallel execution load    |\n",
    "| Failure Rate | Retries, errors, timeouts  |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Instrumentation Architecture**\n",
    "\n",
    "```\n",
    "Client\n",
    "  |\n",
    "LangGraph Runtime\n",
    "  |\n",
    "Execution Tracer\n",
    "  |—— Node Timings\n",
    "  |—— State Snapshots\n",
    "  |—— Token Counters\n",
    "  |—— Tool Metrics\n",
    "  |\n",
    "Observability Backend\n",
    "(LangSmith / OpenTelemetry / Prometheus)\n",
    "```\n",
    "\n",
    "LangGraph exposes **hooks** for each execution stage.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Enabling Profiling in LangGraph**\n",
    "\n",
    "```python\n",
    "from langchain.callbacks import LangChainTracer\n",
    "from langsmith import Client\n",
    "\n",
    "tracer = LangChainTracer()\n",
    "result = graph.invoke(input, config={\"callbacks\": [tracer]})\n",
    "```\n",
    "\n",
    "This automatically records:\n",
    "\n",
    "* Node entry/exit\n",
    "* Token counts\n",
    "* Tool latency\n",
    "* State transitions\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Node-Level Timing Analysis**\n",
    "\n",
    "Each node produces timing metadata:\n",
    "\n",
    "| Metric      | Meaning                 |\n",
    "| ----------- | ----------------------- |\n",
    "| start_time  | Node execution start    |\n",
    "| end_time    | Node execution end      |\n",
    "| duration    | end_time - start_time   |\n",
    "| retry_count | Failures before success |\n",
    "\n",
    "Used to detect:\n",
    "\n",
    "* Slow LLM calls\n",
    "* Tool bottlenecks\n",
    "* Infinite loops\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Token & Cost Profiling**\n",
    "\n",
    "```python\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    graph.invoke(input)\n",
    "\n",
    "print(cb.total_tokens, cb.total_cost)\n",
    "```\n",
    "\n",
    "| Metric            | Purpose             |\n",
    "| ----------------- | ------------------- |\n",
    "| Prompt tokens     | Context size        |\n",
    "| Completion tokens | Output size         |\n",
    "| Total cost        | Budget control      |\n",
    "| Calls per run     | Throughput planning |\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Profiling Cyclic & Agentic Workflows**\n",
    "\n",
    "Cyclic graphs require **loop-aware profiling**.\n",
    "\n",
    "Metrics tracked per iteration:\n",
    "\n",
    "* Iteration count\n",
    "* Cumulative latency\n",
    "* Convergence rate\n",
    "* Error accumulation\n",
    "\n",
    "```python\n",
    "graph.invoke(input, config={\"recursion_limit\": 20})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Visualization & Debugging**\n",
    "\n",
    "| Tool         | Capability                       |\n",
    "| ------------ | -------------------------------- |\n",
    "| LangSmith    | Execution timelines, token usage |\n",
    "| Graph Viewer | Structural bottlenecks           |\n",
    "| Tracing UI   | State evolution                  |\n",
    "| Logs         | Failure diagnosis                |\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Performance Optimization Techniques**\n",
    "\n",
    "| Bottleneck    | Solution               |\n",
    "| ------------- | ---------------------- |\n",
    "| Slow LLM      | Model routing, caching |\n",
    "| Large prompts | State pruning          |\n",
    "| Slow tools    | Async + batching       |\n",
    "| High cost     | Token compression      |\n",
    "| Long loops    | Convergence rules      |\n",
    "| Memory bloat  | Checkpoint pruning     |\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Production Performance Guardrails**\n",
    "\n",
    "| Guardrail        | Purpose                |\n",
    "| ---------------- | ---------------------- |\n",
    "| Max recursion    | Prevent infinite loops |\n",
    "| Timeouts         | Kill hung nodes        |\n",
    "| Rate limits      | Prevent overload       |\n",
    "| Budget caps      | Enforce cost ceiling   |\n",
    "| Circuit breakers | Stop failure cascades  |\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Performance Testing Workflow**\n",
    "\n",
    "```\n",
    "Define SLA → Instrument Graph → Run Load Tests →\n",
    "Collect Metrics → Identify Bottlenecks → Optimize → Re-test\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **12. Mental Model**\n",
    "\n",
    "> **LangGraph performance profiling treats your AI system like a distributed service.**\n",
    "\n",
    "You do not optimize prompts.\n",
    "You optimize **systems**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b079688-0f4e-4733-bd26-d32b01ed7b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-1369-7821-abe6-267a6454274f; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-1626-71a3-9247-635de9c1eb20; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-1627-7261-bb87-2b1b216824fc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Reason Node] Time: 1.53s\n",
      "[Check Node] Step: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-1627-7261-bb87-2b1b216824fc; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-1626-71a3-9247-635de9c1eb20; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-1c1f-73a1-9f0c-0b0eb3d1d464; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-1c20-7d01-8abe-a819c4329cc4; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-1c20-7d01-8abe-a819c4329cc4; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-1c1f-73a1-9f0c-0b0eb3d1d464; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-1c21-73f1-99da-19984461004e; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-1c22-7401-885c-54bd88f20ca7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Reason Node] Time: 2.02s\n",
      "[Check Node] Step: 2\n",
      "\n",
      "=== FINAL RESULT ===\n",
      "{'question': 'Why is the sky blue?', 'answer': \"The sky appears blue due to Rayleigh scattering. When sunlight enters the Earth's atmosphere, shorter blue wavelengths are scattered in all directions by air molecules, making the sky look blue to our eyes.\", 'step': 2}\n",
      "\n",
      "=== TOKEN & COST REPORT ===\n",
      "Total Tokens: 108\n",
      "Prompt Tokens: 32\n",
      "Completion Tokens: 76\n",
      "Estimated Cost: $0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unauthorized\"}\\n')trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-1c22-7401-885c-54bd88f20ca7; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-1c21-73f1-99da-19984461004e; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-240b-7920-8b6a-8aa28dc37dcb; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-240c-7be1-9d71-53433d0adc3c; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-240c-7be1-9d71-53433d0adc3c; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-240b-7920-8b6a-8aa28dc37dcb; trace=019b6e0e-1369-7821-abe6-267a6454274f,id=019b6e0e-1369-7821-abe6-267a6454274f\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# LangGraph Performance Profiling Demo (Single Cell)\n",
    "# =========================\n",
    "\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.callbacks import LangChainTracer, get_openai_callback\n",
    "import time\n",
    "\n",
    "# -------- 1. Define shared state --------\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    step: int\n",
    "\n",
    "# -------- 2. Instrumented Nodes --------\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def reason_node(state: State):\n",
    "    start = time.time()\n",
    "    response = llm.invoke(f\"Answer briefly: {state['question']}\")\n",
    "    duration = time.time() - start\n",
    "    print(f\"[Reason Node] Time: {duration:.2f}s\")\n",
    "    return {\n",
    "        \"answer\": response.content,\n",
    "        \"step\": state[\"step\"] + 1\n",
    "    }\n",
    "\n",
    "def check_node(state: State):\n",
    "    print(f\"[Check Node] Step: {state['step']}\")\n",
    "    if state[\"step\"] >= 2:\n",
    "        return {\"done\": True}\n",
    "    return {\"done\": False}\n",
    "\n",
    "# -------- 3. Build Cyclic Graph --------\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"reason\", reason_node)\n",
    "builder.add_node(\"check\", check_node)\n",
    "\n",
    "builder.set_entry_point(\"reason\")\n",
    "builder.add_edge(\"reason\", \"check\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"check\",\n",
    "    lambda s: END if s[\"done\"] else \"reason\",\n",
    "    {\"reason\": \"reason\", END: END}\n",
    ")\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# -------- 4. Enable Tracing & Token Profiling --------\n",
    "\n",
    "tracer = LangChainTracer()\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = graph.invoke(\n",
    "        {\"question\": \"Why is the sky blue?\", \"step\": 0},\n",
    "        config={\"callbacks\": [tracer], \"recursion_limit\": 5}\n",
    "    )\n",
    "\n",
    "# -------- 5. Performance Report --------\n",
    "\n",
    "print(\"\\n=== FINAL RESULT ===\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\n=== TOKEN & COST REPORT ===\")\n",
    "print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "print(f\"Estimated Cost: ${cb.total_cost:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
