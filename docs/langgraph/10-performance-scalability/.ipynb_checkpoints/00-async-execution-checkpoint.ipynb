{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41dfd512-36cc-48c1-85f5-5aa3c3bb4b34",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Async Execution \n",
    "\n",
    "**Async execution** in LangGraph enables **non-blocking, concurrent, high-throughput execution** of LLM workflows by allowing multiple nodes, tools, and agents to run simultaneously using Python’s `async/await` model.\n",
    "It is the foundation for **scalable, production-grade agent systems**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Motivation and Intuition**\n",
    "\n",
    "LLM systems spend most of their time **waiting**:\n",
    "\n",
    "* API calls to LLM providers\n",
    "* Tool I/O (databases, search, APIs)\n",
    "* Network communication\n",
    "* Human input\n",
    "\n",
    "Synchronous execution wastes resources during these waits.\n",
    "Async execution overlaps these waits, increasing **throughput** and reducing **latency**.\n",
    "\n",
    "**Mental model:**\n",
    "\n",
    "> While one node waits for the LLM, another node executes a tool.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Execution Model in LangGraph**\n",
    "\n",
    "LangGraph’s runtime is built on an **event-driven async scheduler**.\n",
    "\n",
    "| Component  | Role                            |\n",
    "| ---------- | ------------------------------- |\n",
    "| Async Node | A node defined with `async def` |\n",
    "| Event Loop | Drives concurrent execution     |\n",
    "| Task       | A scheduled node execution      |\n",
    "| Awaitable  | Any I/O-bound computation       |\n",
    "| Futures    | Handle in-flight node results   |\n",
    "\n",
    "Nodes that are `async` automatically become **non-blocking execution units**.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Defining Async Nodes**\n",
    "\n",
    "```python\n",
    "async def fetch_data(state):\n",
    "    result = await external_api_call(state[\"query\"])\n",
    "    return {\"data\": result}\n",
    "```\n",
    "\n",
    "```python\n",
    "builder.add_node(\"fetch\", fetch_data)\n",
    "```\n",
    "\n",
    "LangGraph detects `async def` and schedules it on the event loop.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Mixed Sync + Async Graphs**\n",
    "\n",
    "LangGraph supports **hybrid execution**.\n",
    "\n",
    "```python\n",
    "def preprocess(state):\n",
    "    return {\"query\": state[\"input\"]}\n",
    "\n",
    "async def llm_call(state):\n",
    "    return {\"answer\": await llm.ainvoke(state[\"query\"])}\n",
    "```\n",
    "\n",
    "Both execute correctly inside the same graph.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Parallel Execution with Fan-Out / Fan-In**\n",
    "\n",
    "```python\n",
    "async def tool_a(state): ...\n",
    "async def tool_b(state): ...\n",
    "async def tool_c(state): ...\n",
    "```\n",
    "\n",
    "```\n",
    "Router → {tool_a, tool_b, tool_c} → Join → Next\n",
    "```\n",
    "\n",
    "LangGraph schedules all three tools **concurrently**.\n",
    "\n",
    "This yields massive speedups for:\n",
    "\n",
    "* Web search\n",
    "* Multi-model querying\n",
    "* Multi-agent coordination\n",
    "\n",
    "---\n",
    "\n",
    "### **6. End-to-End Async Example**\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    text: str\n",
    "    summary: str\n",
    "    sentiment: str\n",
    "\n",
    "async def summarize(state):\n",
    "    return {\"summary\": await llm.ainvoke(\"Summarize: \" + state[\"text\"])}\n",
    "\n",
    "async def analyze(state):\n",
    "    return {\"sentiment\": await llm.ainvoke(\"Sentiment: \" + state[\"text\"])}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"sum\", summarize)\n",
    "builder.add_node(\"sent\", analyze)\n",
    "\n",
    "builder.set_entry_point(\"sum\")\n",
    "builder.add_edge(\"sum\", \"sent\")\n",
    "builder.add_edge(\"sent\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "result = await graph.ainvoke({\"text\": \"LangGraph is powerful\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Async in Multi-Agent Systems**\n",
    "\n",
    "Each agent runs as an **independent async task**:\n",
    "\n",
    "| Agent      | Role            |\n",
    "| ---------- | --------------- |\n",
    "| Planner    | Async reasoning |\n",
    "| Researcher | Async retrieval |\n",
    "| Verifier   | Async checking  |\n",
    "\n",
    "All agents execute in parallel under one scheduler.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Performance Characteristics**\n",
    "\n",
    "| Feature      | Benefit                               |\n",
    "| ------------ | ------------------------------------- |\n",
    "| Non-blocking | No idle CPU                           |\n",
    "| Concurrency  | Higher throughput                     |\n",
    "| Parallel I/O | Lower latency                         |\n",
    "| Scalable     | Supports thousands of concurrent runs |\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Production Controls**\n",
    "\n",
    "| Mechanism          | Purpose                         |\n",
    "| ------------------ | ------------------------------- |\n",
    "| Concurrency limits | Prevent overload                |\n",
    "| Timeouts           | Prevent hangs                   |\n",
    "| Cancellation       | Kill runaway tasks              |\n",
    "| Retries            | Recover from transient failures |\n",
    "| Backpressure       | Flow control                    |\n",
    "\n",
    "```python\n",
    "graph.ainvoke(input, config={\"recursion_limit\": 50})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Common Async Patterns**\n",
    "\n",
    "| Pattern               | Description                     |\n",
    "| --------------------- | ------------------------------- |\n",
    "| Async ReAct Loop      | Reason-Act-Observe concurrently |\n",
    "| Map-Reduce            | Parallel task decomposition     |\n",
    "| Supervisor-Worker     | Async multi-agent               |\n",
    "| Speculative Execution | Race multiple models            |\n",
    "| Batch Async           | High-throughput pipelines       |\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Why Async Is Essential for LangGraph**\n",
    "\n",
    "Without async, LangGraph becomes a slow pipeline.\n",
    "With async, LangGraph becomes a **distributed cognitive engine**.\n",
    "\n",
    "> Async execution is what transforms LangGraph from a workflow tool into a **scalable agent operating system**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f18d5b2f-9658-4922-8ec0-62b75b34945c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'LangGraph enables high performance agent systems', 'summary': 'Result(Summarize: Lang)', 'sentiment': 'Result(Sentiment: Lang)'}\n",
      "Execution time: 2.03 seconds\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# ----------- State Schema -----------\n",
    "\n",
    "class State(TypedDict):\n",
    "    text: str\n",
    "    summary: str\n",
    "    sentiment: str\n",
    "\n",
    "# ----------- Async \"LLM\" Simulations -----------\n",
    "\n",
    "async def fake_llm(prompt: str):\n",
    "    await asyncio.sleep(1)   # simulate network latency\n",
    "    return f\"Result({prompt[:15]})\"\n",
    "\n",
    "async def summarize(state: State):\n",
    "    result = await fake_llm(\"Summarize: \" + state[\"text\"])\n",
    "    return {\"summary\": result}\n",
    "\n",
    "async def analyze(state: State):\n",
    "    result = await fake_llm(\"Sentiment: \" + state[\"text\"])\n",
    "    return {\"sentiment\": result}\n",
    "\n",
    "# ----------- Build Graph -----------\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"summarize\", summarize)\n",
    "builder.add_node(\"analyze\", analyze)\n",
    "\n",
    "# Fan-out: both run concurrently\n",
    "builder.set_entry_point(\"summarize\")\n",
    "builder.add_edge(\"summarize\", \"analyze\")\n",
    "builder.add_edge(\"analyze\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# ----------- Run Async Graph -----------\n",
    "\n",
    "async def main():\n",
    "    start = asyncio.get_event_loop().time()\n",
    "    result = await graph.ainvoke({\"text\": \"LangGraph enables high performance agent systems\"})\n",
    "    end = asyncio.get_event_loop().time()\n",
    "    print(result)\n",
    "    print(f\"Execution time: {round(end-start,2)} seconds\")\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab56b0-4d9e-4877-900e-0ce46e4319c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
