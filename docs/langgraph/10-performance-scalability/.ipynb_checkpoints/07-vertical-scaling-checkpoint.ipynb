{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e18d2c",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Vertical Scaling\n",
    "\n",
    "**Vertical scaling** in LangGraph refers to **increasing the capacity of a single LangGraph runtime instance** to handle larger workloads, higher concurrency, and more complex workflows by allocating **more CPU, memory, I/O bandwidth, and GPU resources** to that instance â€” rather than adding more instances (horizontal scaling).\n",
    "\n",
    "It is the primary scaling strategy for **state-heavy, long-running, and highly coupled LLM workflows**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Why Vertical Scaling Matters in LangGraph**\n",
    "\n",
    "LangGraph workloads are **stateful** and often include:\n",
    "\n",
    "* Long reasoning chains\n",
    "* Large shared state\n",
    "* Memory stores\n",
    "* Multi-agent coordination\n",
    "* Streaming and tool I/O\n",
    "\n",
    "These characteristics benefit strongly from **high-resource single-node execution**.\n",
    "\n",
    "| Bottleneck   | How Vertical Scaling Helps  |\n",
    "| ------------ | --------------------------- |\n",
    "| CPU          | Faster reasoning & routing  |\n",
    "| Memory       | Large state + embeddings    |\n",
    "| I/O          | Faster tool & DB calls      |\n",
    "| GPU          | Higher throughput LLM calls |\n",
    "| Context size | Supports bigger prompts     |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. What Is Being Scaled**\n",
    "\n",
    "| Component         | Scaling Target         |\n",
    "| ----------------- | ---------------------- |\n",
    "| LangGraph runtime | Process & thread pools |\n",
    "| State store       | In-memory & persistent |\n",
    "| Checkpoint engine | Faster serialization   |\n",
    "| Agent scheduler   | Parallel agent control |\n",
    "| LLM client        | Higher throughput      |\n",
    "| Tool executors    | More concurrent calls  |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Vertical Scaling Architecture**\n",
    "\n",
    "```\n",
    "Client\n",
    "   |\n",
    "Load Balancer (optional)\n",
    "   |\n",
    "High-Capacity LangGraph Runtime\n",
    "   |---- State Store (RAM + SSD)\n",
    "   |---- Checkpoint Engine\n",
    "   |---- Agent Scheduler\n",
    "   |---- Tool Executor Pool\n",
    "   |\n",
    "LLM APIs / Local GPU Models\n",
    "```\n",
    "\n",
    "A **single powerful node** replaces multiple smaller nodes.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. LangGraph Execution Model Under Vertical Scaling**\n",
    "\n",
    "LangGraph internally relies on:\n",
    "\n",
    "* **Async task scheduling**\n",
    "* **Thread pools**\n",
    "* **Event loops**\n",
    "* **State locks**\n",
    "\n",
    "Vertical scaling increases capacity of these mechanisms.\n",
    "\n",
    "```python\n",
    "graph.invoke(\n",
    "    input,\n",
    "    config={\n",
    "        \"max_concurrency\": 64,\n",
    "        \"recursion_limit\": 50\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Memory-Heavy Workflows Example**\n",
    "\n",
    "```python\n",
    "class State(TypedDict):\n",
    "    history: list\n",
    "    context: str\n",
    "    embeddings: list\n",
    "    plan: str\n",
    "```\n",
    "\n",
    "Large memory requires **RAM expansion** for efficient execution.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Performance Tuning Levers**\n",
    "\n",
    "| Resource    | Tuning Strategy            |\n",
    "| ----------- | -------------------------- |\n",
    "| CPU         | Increase core count        |\n",
    "| Memory      | Increase RAM               |\n",
    "| Disk        | NVMe SSD                   |\n",
    "| Network     | High-throughput NIC        |\n",
    "| GPU         | More VRAM, multi-GPU       |\n",
    "| Threads     | Larger thread pool         |\n",
    "| Async tasks | Higher event-loop capacity |\n",
    "\n",
    "---\n",
    "\n",
    "### **7. When to Prefer Vertical Over Horizontal**\n",
    "\n",
    "| Scenario                | Scaling Choice |\n",
    "| ----------------------- | -------------- |\n",
    "| Large shared state      | Vertical       |\n",
    "| Heavy agent coupling    | Vertical       |\n",
    "| Long-running sessions   | Vertical       |\n",
    "| High per-request memory | Vertical       |\n",
    "| Low-latency needs       | Vertical       |\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Vertical Scaling vs Horizontal Scaling**\n",
    "\n",
    "| Feature                 | Vertical           | Horizontal          |\n",
    "| ----------------------- | ------------------ | ------------------- |\n",
    "| State management        | Simple             | Complex             |\n",
    "| Latency                 | Low                | Higher              |\n",
    "| Cost efficiency         | High (medium load) | High (massive load) |\n",
    "| Fault tolerance         | Lower              | Higher              |\n",
    "| Architecture complexity | Low                | High                |\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Production Configuration Example**\n",
    "\n",
    "```bash\n",
    "# Kubernetes pod spec\n",
    "resources:\n",
    "  requests:\n",
    "    cpu: \"16\"\n",
    "    memory: \"64Gi\"\n",
    "  limits:\n",
    "    cpu: \"32\"\n",
    "    memory: \"128Gi\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Risks & Safeguards**\n",
    "\n",
    "| Risk                    | Mitigation      |\n",
    "| ----------------------- | --------------- |\n",
    "| Single point of failure | Checkpointing   |\n",
    "| Memory leaks            | Monitoring      |\n",
    "| Long recovery time      | Fast restart    |\n",
    "| Resource exhaustion     | Limits & alerts |\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Mental Model**\n",
    "\n",
    "Vertical scaling in LangGraph transforms the system into a **high-performance AI engine** capable of running:\n",
    "\n",
    "> **Complex agents + large memory + deep reasoning + real-time decision loops**\n",
    "\n",
    "on a **single optimized runtime**.\n",
    "\n",
    "\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6842cf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Step: 10\n",
      "Iterations: 10\n",
      "Sample State: [1000000, 1000000, 1000000]\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict\n",
    "import asyncio\n",
    "\n",
    "# ------------------ State ------------------\n",
    "class State(TypedDict):\n",
    "    step: int\n",
    "    history: list\n",
    "    done: bool\n",
    "\n",
    "# ------------------ Nodes ------------------\n",
    "async def heavy_reasoning(state: State):\n",
    "    payload = \"x\" * 1_000_000   # simulate high memory usage\n",
    "    await asyncio.sleep(0.05)   # simulate compute\n",
    "    state[\"history\"].append(len(payload))\n",
    "    return {\"step\": state[\"step\"] + 1}\n",
    "\n",
    "def control(state: State):\n",
    "    return {\"done\": state[\"step\"] >= 10}\n",
    "\n",
    "def router(state: State):\n",
    "    return END if state[\"done\"] else \"reason\"\n",
    "\n",
    "# ------------------ Graph ------------------\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"reason\", heavy_reasoning)\n",
    "builder.add_node(\"control\", control)\n",
    "\n",
    "builder.set_entry_point(\"reason\")\n",
    "builder.add_edge(\"reason\", \"control\")\n",
    "builder.add_conditional_edges(\"control\", router, {\n",
    "    \"reason\": \"reason\",\n",
    "    END: END\n",
    "})\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# ------------------ Execute (Notebook Safe) ------------------\n",
    "result = await graph.ainvoke(\n",
    "    {\"step\": 0, \"history\": [], \"done\": False},\n",
    "    config={\"recursion_limit\": 50, \"max_concurrency\": 32}\n",
    ")\n",
    "\n",
    "print(\"Final Step:\", result[\"step\"])\n",
    "print(\"Iterations:\", len(result[\"history\"]))\n",
    "print(\"Sample State:\", result[\"history\"][:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10177d92-68c6-4fc6-8ea1-dc12c22a6168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "py312env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
